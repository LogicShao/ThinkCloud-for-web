# ThinkCloud for Web æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## 1. æ¦‚è¿°

æœ¬æŒ‡å—æ—¨åœ¨ä¸º ThinkCloud for Web é¡¹ç›®æä¾›å…¨é¢çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬APIè°ƒç”¨ä¼˜åŒ–ã€ç¼“å­˜æœºåˆ¶ã€å‰ç«¯æ€§èƒ½ã€èµ„æºç®¡ç†ç­‰æ–¹é¢ã€‚

## 2. APIè°ƒç”¨æ€§èƒ½ä¼˜åŒ–

### 2.1 è¿æ¥æ± å’Œä¼šè¯å¤ç”¨

- **é—®é¢˜**: å½“å‰å®ç°ä¸­ï¼Œæ¯æ¬¡APIè°ƒç”¨éƒ½å¯èƒ½åˆ›å»ºæ–°çš„è¿æ¥
- **ä¼˜åŒ–**: åœ¨æä¾›å•†å®¢æˆ·ç«¯ä¸­å®ç°è¿æ¥æ± å’Œä¼šè¯å¤ç”¨

```python
# åœ¨ providers.py ä¸­ä¼˜åŒ–å®¢æˆ·ç«¯åˆå§‹åŒ–
from openai import OpenAI
import httpx

class BaseProvider(ABC):
    _http_client = None
    
    def __init__(self, provider_name: str):
        self.provider_name = provider_name
        self.client = None
        # åˆ›å»ºå…±äº«çš„HTTPå®¢æˆ·ç«¯
        if BaseProvider._http_client is None:
            BaseProvider._http_client = httpx.Client(
                timeout=30.0,  # 30ç§’è¶…æ—¶
                limits=httpx.Limits(
                    max_keepalive_connections=20,
                    max_connections=100,
                    keepalive_expiry=300  # 5åˆ†é’Ÿä¿æŒè¿æ¥
                )
            )
        self._initialize_client()
```

### 2.2 è¯·æ±‚å‚æ•°ä¼˜åŒ–

- **é—®é¢˜**: å½“å‰å¯¹æ‰€æœ‰å‚æ•°éƒ½è¿›è¡Œä¼ é€’ï¼Œå¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—
- **ä¼˜åŒ–**: åªä¼ é€’éé»˜è®¤å€¼çš„å‚æ•°

```python
# åœ¨ providers.py ä¸­ä¼˜åŒ–å‚æ•°ä¼ é€’
def _build_api_params(self, **kwargs):
    """æ„å»ºAPIå‚æ•°ï¼ŒåªåŒ…å«éé»˜è®¤å€¼"""
    api_params = {"model": kwargs["model"], "messages": kwargs["messages"]}
    
    # åªæ·»åŠ éé»˜è®¤å€¼å‚æ•°
    if kwargs.get("temperature") is not None and kwargs["temperature"] != 0.7:
        api_params["temperature"] = kwargs["temperature"]
    if kwargs.get("top_p") is not None and kwargs["top_p"] != 0.9:
        api_params["top_p"] = kwargs["top_p"]
    if kwargs.get("max_tokens") is not None and kwargs["max_tokens"] != 2048:
        api_params["max_tokens"] = kwargs["max_tokens"]
    
    return api_params
```

## 3. ç¼“å­˜æœºåˆ¶ä¼˜åŒ–

### 3.1 å¯¹è¯å†å²ç¼“å­˜

- **é—®é¢˜**: å¯¹è¯å†å²å®Œå…¨å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œæ²¡æœ‰ç¼“å­˜ç­–ç•¥
- **ä¼˜åŒ–**: å®ç°æ™ºèƒ½ç¼“å­˜å’Œå†å²æˆªæ–­

```python
# åœ¨ chat_manager.py ä¸­ä¼˜åŒ–
import threading
from collections import deque
from datetime import datetime, timedelta

class ChatManager:
    def __init__(self, max_history_length=50, history_ttl_minutes=30):
        self.history = deque(maxlen=max_history_length)  # é™åˆ¶å†å²é•¿åº¦
        self.history_ttl = timedelta(minutes=history_ttl_minutes)
        self.last_access = datetime.now()
        self._lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        with self._lock:
            self.history.append({
                "role": role, 
                "content": content, 
                "timestamp": datetime.now()
            })
            self.last_access = datetime.now()

    def get_recent_messages(self, max_tokens=4000):
        """è·å–æœ€è¿‘çš„æ¶ˆæ¯ï¼Œé™åˆ¶tokenæ•°"""
        with self._lock:
            # å®ç°åŸºäºtokenæ•°çš„æ¶ˆæ¯æˆªæ–­é€»è¾‘
            recent_messages = []
            total_tokens = 0
            
            for msg in reversed(self.history):
                msg_tokens = self._estimate_tokens(msg["content"])
                if total_tokens + msg_tokens > max_tokens:
                    break
                recent_messages.insert(0, msg)
                total_tokens += msg_tokens
            
            return recent_messages

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡æŒ‰å­—ç¬¦æ•°/4ï¼Œä¸­æ–‡æŒ‰å­—ç¬¦æ•°/1.5
        english_chars = sum(1 for c in text if c.isascii() and c.isalnum())
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - english_chars - chinese_chars
        
        return int(english_chars / 4 + chinese_chars / 1.5 + other_chars / 2)
```

### 3.2 APIå“åº”ç¼“å­˜

- **é—®é¢˜**: ç›¸åŒé—®é¢˜å¯èƒ½è¢«é‡å¤è¯·æ±‚
- **ä¼˜åŒ–**: å®ç°å“åº”ç¼“å­˜æœºåˆ¶

```python
# åœ¨ api_service.py ä¸­æ·»åŠ ç¼“å­˜
import hashlib
from functools import lru_cache
from datetime import datetime, timedelta


class CacheManager:
    def __init__(self, ttl_minutes=10):
        self.cache = {}
        self.ttl = timedelta(minutes=ttl_minutes)

    def get(self, key: str):
        """è·å–ç¼“å­˜é¡¹"""
        if key in self.cache:
            data, timestamp = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                return data
            else:
                del self.cache[key]  # æ¸…é™¤è¿‡æœŸé¡¹
        return None

    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜é¡¹"""
        self.cache[key] = (value, datetime.now())

    def generate_key(self, messages, model, **kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_input = {
            'messages': messages,
            'model': model,
            'temperature': kwargs.get('temperature'),
            'top_p': kwargs.get('top_p')
        }
        cache_str = str(sorted(cache_input.items()))
        return hashlib.md5(cache_str.encode()).hexdigest()


# åœ¨ MultiProviderAPIService ä¸­ä½¿ç”¨ç¼“å­˜
class MultiProviderAPIService:
    def __init__(self):
        self.providers = {}
        self.cache_manager = CacheManager()
        self._initialize_providers()

    def chat_completion(self, messages, model, **kwargs):
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.cache_manager.generate_key(messages, model, **kwargs)

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self.cache_manager.get(cache_key)
        if cached_result is not None:
            print(f"[CACHE] ä½¿ç”¨ç¼“å­˜å“åº”")
            return cached_result

        # è°ƒç”¨å®é™…API
        result = self._call_actual_api(messages, model, **kwargs)

        # å­˜å‚¨åˆ°ç¼“å­˜
        self.cache_manager.set(cache_key, result)

        return result
```

## 4. å‰ç«¯æ€§èƒ½ä¼˜åŒ–

### 4.1 æµå¼ä¼ è¾“ä¼˜åŒ–

- **é—®é¢˜**: æµå¼ä¼ è¾“æ—¶é¢‘ç¹æ›´æ–°UIå¯èƒ½å½±å“æ€§èƒ½
- **ä¼˜åŒ–**: æ‰¹é‡æ›´æ–°å’Œé˜²æŠ–æœºåˆ¶

```python
# åœ¨ main.py ä¸­ä¼˜åŒ–æµå¼ä¼ è¾“å¤„ç†
import asyncio
from typing import Generator

def bot_message_with_batching(...):
    """å¸¦æ‰¹é‡æ›´æ–°çš„æµå¼å“åº”å¤„ç†"""
    if enable_stream:
        # æ·»åŠ ç©ºåŠ©æ‰‹æ¶ˆæ¯
        history.append({
            "role": "assistant",
            "content": "",
            "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"}
        })
        
        response_text = ""
        batch_buffer = ""
        batch_size = 10  # æ¯10ä¸ªå­—ç¬¦æ‰¹é‡æ›´æ–°ä¸€æ¬¡
        
        try:
            stream_generator = api_service.chat_completion(
                messages=api_messages,
                model=model,
                system_instruction=actual_sys_inst,
                temperature=temp,
                top_p=top_p_val,
                max_tokens=int(max_tok) if max_tok else None,
                frequency_penalty=freq_pen,
                presence_penalty=pres_pen,
                stream=True
            )
            
            for chunk in stream_generator:
                batch_buffer += chunk
                response_text += chunk
                
                # æ¯ç§¯ç´¯ä¸€å®šå­—ç¬¦æ•°å°±æ›´æ–°UI
                if len(batch_buffer) >= batch_size:
                    history[-1]["content"] = response_text
                    yield history
                    batch_buffer = ""  # æ¸…ç©ºç¼“å†²åŒº
            
            # å¤„ç†å‰©ä½™çš„ç¼“å†²å†…å®¹
            if batch_buffer:
                history[-1]["content"] = response_text
                yield history
            
            # æ·»åŠ å“åº”æ—¶é—´
            response_text = add_duration_to_response(response_text, start_time)
            history[-1]["content"] = response_text
            yield history
            
        except Exception as e:
            # é”™è¯¯å¤„ç†é€»è¾‘
            pass
```

### 4.2 ç•Œé¢å“åº”ä¼˜åŒ–

- **é—®é¢˜**: é•¿å¯¹è¯å†å²å¯èƒ½å¯¼è‡´ç•Œé¢å¡é¡¿
- **ä¼˜åŒ–**: è™šæ‹Ÿæ»šåŠ¨å’Œå†å²åˆ†é¡µ

## 5. æ·±åº¦æ€è€ƒæ¨¡å¼ä¼˜åŒ–

### 5.1 å¹¶è¡Œå¤„ç†å­ä»»åŠ¡

- **é—®é¢˜**: å½“å‰æ·±åº¦æ€è€ƒæ˜¯ä¸²è¡Œå¤„ç†å­ä»»åŠ¡
- **ä¼˜åŒ–**: åœ¨å¯èƒ½çš„æƒ…å†µä¸‹å¹¶è¡Œå¤„ç†ç‹¬ç«‹å­ä»»åŠ¡

```python
# åœ¨ deep_think.py ä¸­ä¼˜åŒ–
import asyncio
from concurrent.futures import ThreadPoolExecutor

class DeepThinkOrchestrator:
    def __init__(self, ...):
        # ... å…¶ä»–åˆå§‹åŒ–
        self.executor = ThreadPoolExecutor(max_workers=3)  # é™åˆ¶å¹¶å‘æ•°
    
    async def _solve_subtasks_parallel(self, question: str, plan: Plan) -> List[SubtaskResult]:
        """å¹¶è¡Œè§£å†³æ— ä¾èµ–çš„å­ä»»åŠ¡"""
        subtask_results = []
        
        # æŒ‰ä¾èµ–å…³ç³»åˆ†ç»„
        task_groups = self._group_by_dependencies(plan.subtasks)
        
        for group in task_groups:
            # å¹¶è¡Œå¤„ç†åŒä¸€ç»„çš„å­ä»»åŠ¡
            tasks = [
                asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self._solve_single_subtask_sync,
                    subtask,
                    question,
                    subtask_results
                )
                for subtask in group
            ]
            
            group_results = await asyncio.gather(*tasks)
            subtask_results.extend(group_results)
        
        return subtask_results
    
    def _group_by_dependencies(self, subtasks: List[Subtask]) -> List[List[Subtask]]:
        """æ ¹æ®ä¾èµ–å…³ç³»å¯¹å­ä»»åŠ¡åˆ†ç»„"""
        # å®ç°ä¾èµ–åˆ†æå’Œåˆ†ç»„é€»è¾‘
        groups = []
        remaining_tasks = subtasks.copy()
        
        while remaining_tasks:
            group = []
            to_remove = []
            
            for task in remaining_tasks:
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä¾èµ–ä»»åŠ¡éƒ½å·²å¤„ç†
                all_deps_resolved = all(
                    any(dep_id == dep_task.id for dep_task in processed_tasks)
                    for dep_id in task.dependencies
                )
                
                if all_deps_resolved:
                    group.append(task)
                    to_remove.append(task)
            
            if not group:  # é˜²æ­¢æ­»å¾ªç¯
                break
                
            groups.append(group)
            for task in to_remove:
                remaining_tasks.remove(task)
        
        return groups
```

### 5.2 ä¸­é—´ç»“æœç¼“å­˜

- **é—®é¢˜**: æ·±åº¦æ€è€ƒçš„ä¸­é—´æ­¥éª¤å¯èƒ½è¢«é‡å¤è®¡ç®—
- **ä¼˜åŒ–**: ç¼“å­˜ä¸­é—´ç»“æœ

```python
class DeepThinkOrchestrator:
    def __init__(self, ...):
        # ... å…¶ä»–åˆå§‹åŒ–
        self.intermediate_cache = {}  # ä¸­é—´ç»“æœç¼“å­˜
    
    def _get_cache_key(self, method_name: str, *args, **kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_input = {
            'method': method_name,
            'args': args,
            'kwargs': {k: v for k, v in kwargs.items() if k != 'self'}
        }
        cache_str = str(sorted(cache_input.items()))
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _plan_with_cache(self, question: str) -> Plan:
        """å¸¦ç¼“å­˜çš„è§„åˆ’æ–¹æ³•"""
        cache_key = self._get_cache_key('_plan', question)
        cached = self.intermediate_cache.get(cache_key)
        
        if cached:
            return cached
        
        result = self._plan(question)
        self.intermediate_cache[cache_key] = result
        return result
```

## 6. å†…å­˜ç®¡ç†ä¼˜åŒ–

### 6.1 å¯¹è±¡å¤ç”¨

- **é—®é¢˜**: é¢‘ç¹åˆ›å»ºå’Œé”€æ¯å¯¹è±¡
- **ä¼˜åŒ–**: å¯¹è±¡æ± æ¨¡å¼

```python
# å®ç°ç®€å•çš„å¯¹è±¡æ± 
class MessageProcessorPool:
    def __init__(self, initial_size=5):
        self.pool = [MessageProcessor() for _ in range(initial_size)]
        self.lock = threading.Lock()
    
    def get_processor(self):
        with self.lock:
            if self.pool:
                return self.pool.pop()
            else:
                return MessageProcessor()  # å¦‚æœæ± ç©ºåˆ™åˆ›å»ºæ–°çš„
    
    def return_processor(self, processor):
        with self.lock:
            if len(self.pool) < 10:  # é™åˆ¶æ± å¤§å°
                self.pool.append(processor)
```

## 7. é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥

### 7.1 APIé™çº§

- **é—®é¢˜**: æŸä¸ªæä¾›å•†ä¸å¯ç”¨æ—¶å½±å“æ•´ä½“æœåŠ¡
- **ä¼˜åŒ–**: å®ç°APIé™çº§å’Œè‡ªåŠ¨åˆ‡æ¢

```python
class MultiProviderAPIService:
    def chat_completion_with_fallback(self, messages, model, **kwargs):
        """å¸¦é™çº§æœºåˆ¶çš„APIè°ƒç”¨"""
        primary_provider = get_model_provider(model)

        # é¦–å…ˆå°è¯•ä¸»è¦æä¾›å•†
        if self.is_available(primary_provider):
            try:
                return self.chat_completion(messages, model, **kwargs)
            except Exception as e:
                print(f"[FALLBACK] {primary_provider} å¤±è´¥: {e}")

        # å°è¯•å…¶ä»–å¯ç”¨æä¾›å•†
        fallback_providers = [p for p in self.providers.keys() if p != primary_provider]
        for provider in fallback_providers:
            if self.is_available(provider):
                try:
                    # è·å–è¯¥æä¾›å•†çš„å…¼å®¹æ¨¡å‹
                    compatible_model = self._get_compatible_model(provider, model)
                    if compatible_model:
                        print(f"[FALLBACK] åˆ‡æ¢åˆ° {provider}")
                        return self.chat_completion(messages, compatible_model, **kwargs)
                except Exception as e:
                    print(f"[FALLBACK] {provider} ä¹Ÿå¤±è´¥: {e}")
                    continue

        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        return "æ‰€æœ‰AIæä¾›å•†å½“å‰éƒ½ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
```

## 8. ç›‘æ§å’Œæ€§èƒ½åˆ†æ

### 8.1 æ€§èƒ½æŒ‡æ ‡æ”¶é›†

- **é—®é¢˜**: ç¼ºä¹æ€§èƒ½ç›‘æ§
- **ä¼˜åŒ–**: æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```python
import time
import statistics
from collections import defaultdict


class PerformanceMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)

    def record_api_call(self, provider: str, duration: float, tokens: int = None):
        """è®°å½•APIè°ƒç”¨æ€§èƒ½"""
        self.metrics[f"{provider}_response_time"].append(duration)
        if tokens:
            self.metrics[f"{provider}_tokens_per_second"].append(tokens / duration if duration > 0 else 0)

    def get_stats(self, provider: str):
        """è·å–æä¾›å•†æ€§èƒ½ç»Ÿè®¡"""
        response_times = self.metrics.get(f"{provider}_response_time", [])
        if not response_times:
            return {}

        return {
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "p95_response_time": sorted(response_times)[int(0.95 * len(response_times))] if response_times else 0
        }


# åœ¨ api_service.py ä¸­é›†æˆç›‘æ§
class MultiProviderAPIService:
    def __init__(self):
        # ... å…¶ä»–åˆå§‹åŒ–
        self.monitor = PerformanceMonitor()

    def chat_completion(self, ...):
        start_time = time.time()
        try:
            result = self._call_actual_api(...)
            duration = time.time() - start_time
            self.monitor.record_api_call(provider_name, duration)
            return result
        except Exception as e:
            duration = time.time() - start_time
            self.monitor.record_api_call(provider_name, duration)
            raise e
```

## 9. é…ç½®ä¼˜åŒ–å»ºè®®

### 9.1 ç³»ç»Ÿé…ç½®

- è°ƒæ•´Gradioé˜Ÿåˆ—è®¾ç½®ä»¥æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›
- ä¼˜åŒ–æœåŠ¡å™¨è¶…æ—¶è®¾ç½®
- é…ç½®åˆé€‚çš„çº¿ç¨‹æ± å¤§å°

### 9.2 ç¯å¢ƒå˜é‡ä¼˜åŒ–

```bash
# .env ç¤ºä¾‹ä¼˜åŒ–é…ç½®
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
GRADIO_NUM_WORKERS=4  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
GRADIO_ENABLE_WEBSOCKETS=true
PYTHONPATH=/workspace
```

## 10. æ€»ç»“

é€šè¿‡å®æ–½ä»¥ä¸Šä¼˜åŒ–æªæ–½ï¼Œå¯ä»¥æ˜¾è‘—æå‡ThinkCloud for Webåº”ç”¨çš„æ€§èƒ½ï¼š

1. **APIæ€§èƒ½**: é€šè¿‡è¿æ¥æ± å’Œå‚æ•°ä¼˜åŒ–å‡å°‘APIè°ƒç”¨æ—¶é—´
2. **ç¼“å­˜ç­–ç•¥**: é€šè¿‡æ™ºèƒ½ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
3. **å‰ç«¯ä½“éªŒ**: é€šè¿‡æ‰¹é‡æ›´æ–°å’Œé˜²æŠ–æœºåˆ¶æå‡ç”¨æˆ·ä½“éªŒ
4. **æ·±åº¦æ€è€ƒ**: é€šè¿‡å¹¶è¡Œå¤„ç†å’Œä¸­é—´ç»“æœç¼“å­˜åŠ é€Ÿå¤æ‚æ¨ç†
5. **ç¨³å®šæ€§**: é€šè¿‡é™çº§ç­–ç•¥å’Œé”™è¯¯å¤„ç†æé«˜ç³»ç»Ÿç¨³å®šæ€§
6. **ç›‘æ§**: é€šè¿‡æ€§èƒ½æŒ‡æ ‡æ”¶é›†æŒç»­ä¼˜åŒ–ç³»ç»Ÿ

è¿™äº›ä¼˜åŒ–æªæ–½åº”è¯¥æ ¹æ®å®é™…ä½¿ç”¨åœºæ™¯å’Œæ€§èƒ½ç“¶é¢ˆé€æ­¥å®æ–½ã€‚