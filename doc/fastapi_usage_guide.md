# FastAPI æœ¬åœ° LLM å®¢æˆ·ç«¯ä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

ThinkCloud FastAPI æœåŠ¡æä¾›äº†ä¸€ä¸ªå®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼çš„æœ¬åœ° LLM
å®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒå¤šæä¾›å•†ï¼ˆCerebrasã€DeepSeekã€OpenAIã€DashScopeã€Kimiï¼‰å’Œ 35+ æ¨¡å‹ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

æ–°å¢ä¾èµ–ï¼š

- `fastapi>=0.104.0` - Web æ¡†æ¶
- `uvicorn[standard]>=0.24.0` - ASGI æœåŠ¡å™¨
- `pydantic>=2.0.0` - æ•°æ®éªŒè¯

### 2. é…ç½®ç¯å¢ƒå˜é‡

ç¡®ä¿ `.env` æ–‡ä»¶ä¸­é…ç½®äº†è‡³å°‘ä¸€ä¸ªæä¾›å•†çš„ API å¯†é’¥ï¼š

```env
CEREBRAS_API_KEY=your_cerebras_api_key
DEEPSEEK_API_KEY=your_deepseek_api_key
OPENAI_API_KEY=your_openai_api_key
DASHSCOPE_API_KEY=your_dashscope_api_key
KIMI_API_KEY=your_kimi_api_key
```

### 3. å¯åŠ¨æœåŠ¡

**æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰**

```bash
python fastapi_main.py
```

**æ–¹å¼äºŒï¼šä½¿ç”¨ uvicorn ç›´æ¥å¯åŠ¨**

```bash
uvicorn src.fastapi_server:app --host 0.0.0.0 --port 8000 --reload
```

æœåŠ¡å¯åŠ¨åä¼šæ˜¾ç¤ºï¼š

```
ğŸš€ ThinkCloud FastAPI Server å¯åŠ¨æˆåŠŸï¼
ğŸ“ åœ°å€: http://localhost:8000
ğŸ“– API æ–‡æ¡£: http://localhost:8000/docs
ğŸ”— OpenAPI Schema: http://localhost:8000/openapi.json
ğŸ’š å¥åº·æ£€æŸ¥: http://localhost:8000/health
ğŸ¤– å¯ç”¨æä¾›å•†: cerebras, deepseek, openai, dashscope, kimi
ğŸ“Š æ¨¡å‹æ€»æ•°: 35
```

### 4. æµ‹è¯•æœåŠ¡

```bash
python test_fastapi.py
```

æµ‹è¯•è„šæœ¬ä¼šè‡ªåŠ¨æµ‹è¯•æ‰€æœ‰ API ç«¯ç‚¹ã€‚

## ğŸ“¡ API ç«¯ç‚¹

### åŸºç¡€ç«¯ç‚¹

#### 1. æ ¹è·¯å¾„

```bash
GET /
```

è¿”å›æœåŠ¡åŸºæœ¬ä¿¡æ¯ã€‚

#### 2. å¥åº·æ£€æŸ¥

```bash
GET /health
```

è¿”å›æœåŠ¡çŠ¶æ€å’Œæä¾›å•†å¯ç”¨æ€§ï¼š

```json
{
  "status": "healthy",
  "providers": {
    "cerebras": true,
    "deepseek": true,
    "openai": false
  },
  "models_count": 35
}
```

### æ¨¡å‹ç®¡ç†

#### 3. åˆ—å‡ºæ‰€æœ‰æ¨¡å‹

```bash
GET /v1/models
```

è¿”å›æ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼‰ï¼š

```json
{
  "object": "list",
  "data": [
    {
      "id": "llama-3.3-70b",
      "object": "model",
      "created": 1234567890,
      "owned_by": "cerebras"
    },
    ...
  ]
}
```

#### 4. è·å–æŒ‡å®šæ¨¡å‹ä¿¡æ¯

```bash
GET /v1/models/{model_id}
```

ç¤ºä¾‹ï¼š

```bash
curl http://localhost:8000/v1/models/llama-3.3-70b
```

### èŠå¤©è¡¥å…¨

#### 5. åˆ›å»ºèŠå¤©è¡¥å…¨ï¼ˆæ ¸å¿ƒç«¯ç‚¹ï¼‰

```bash
POST /v1/chat/completions
```

**è¯·æ±‚æ ¼å¼ï¼ˆOpenAI å…¼å®¹ï¼‰ï¼š**

```json
{
  "model": "llama-3.3-70b",
  "messages": [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
  ],
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 1000,
  "stream": false
}
```

**å‚æ•°è¯´æ˜ï¼š**

- `model` **(å¿…å¡«)**: æ¨¡å‹åç§°ï¼ˆè§ä¸‹æ–¹æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼‰
- `messages` **(å¿…å¡«)**: æ¶ˆæ¯åˆ—è¡¨
    - `role`: `system` / `user` / `assistant`
    - `content`: æ¶ˆæ¯å†…å®¹
- `temperature`: æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰ï¼Œæ§åˆ¶éšæœºæ€§
- `top_p`: æ ¸é‡‡æ ·å‚æ•°ï¼ˆ0.0-1.0ï¼‰
- `max_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°
- `stream`: æ˜¯å¦ä½¿ç”¨æµå¼ä¼ è¾“ï¼ˆtrue/falseï¼‰
- `frequency_penalty`: é¢‘ç‡æƒ©ç½šï¼ˆ-2.0 åˆ° 2.0ï¼‰
- `presence_penalty`: å­˜åœ¨æƒ©ç½šï¼ˆ-2.0 åˆ° 2.0ï¼‰

**éæµå¼å“åº”ï¼š**

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "llama-3.3-70b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

**æµå¼å“åº”ï¼ˆSSE æ ¼å¼ï¼‰ï¼š**

```
data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1234567890,"model":"llama-3.3-70b","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1234567890,"model":"llama-3.3-70b","choices":[{"index":0,"delta":{"content":"ä½ "},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1234567890,"model":"llama-3.3-70b","choices":[{"index":0,"delta":{"content":"å¥½"},"finish_reason":null}]}

...

data: [DONE]
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### Python ç¤ºä¾‹ï¼ˆä½¿ç”¨ requestsï¼‰

#### éæµå¼è¯·æ±‚

```python
import requests
import json

url = "http://localhost:8000/v1/chat/completions"
payload = {
    "model": "llama-3.3-70b",
    "messages": [
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"}
    ],
    "temperature": 0.7,
    "max_tokens": 500,
    "stream": False
}

response = requests.post(url, json=payload)
result = response.json()

print(result['choices'][0]['message']['content'])
```

#### æµå¼è¯·æ±‚

```python
import requests
import json

url = "http://localhost:8000/v1/chat/completions"
payload = {
    "model": "llama-3.3-70b",
    "messages": [
        {"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}
    ],
    "temperature": 0.8,
    "stream": True
}

response = requests.post(url, json=payload, stream=True)

for line in response.iter_lines():
    if line:
        line_text = line.decode('utf-8')
        if line_text.startswith('data: '):
            data_str = line_text[6:]
            if data_str == "[DONE]":
                break
            try:
                chunk = json.loads(data_str)
                delta = chunk['choices'][0]['delta']
                if 'content' in delta:
                    print(delta['content'], end='', flush=True)
            except json.JSONDecodeError:
                pass
```

### ä½¿ç”¨ OpenAI Python SDK

**å®Œå…¨å…¼å®¹ OpenAI SDKï¼**åªéœ€ä¿®æ”¹ `base_url`ï¼š

```python
from openai import OpenAI

# è¿æ¥åˆ°æœ¬åœ° FastAPI æœåŠ¡
client = OpenAI(
    api_key="dummy-key",  # æœ¬åœ°æœåŠ¡ä¸éœ€è¦çœŸå®å¯†é’¥
    base_url="http://localhost:8000/v1"
)

# éæµå¼
response = client.chat.completions.create(
    model="llama-3.3-70b",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    temperature=0.7
)
print(response.choices[0].message.content)

# æµå¼
stream = client.chat.completions.create(
    model="llama-3.3-70b",
    messages=[
        {"role": "user", "content": "å†™ä¸€é¦–è¯—"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='')
```

### cURL ç¤ºä¾‹

#### éæµå¼è¯·æ±‚

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.3-70b",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "temperature": 0.7,
    "stream": false
  }'
```

#### æµå¼è¯·æ±‚

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.3-70b",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "stream": true
  }' \
  --no-buffer
```

## ğŸ¤– æ”¯æŒçš„æ¨¡å‹

### Cerebras (10 ä¸ªæ¨¡å‹)

- `llama-3.3-70b` - æœ€å¿«æ¨ç†é€Ÿåº¦
- `llama-3.1-70b`
- `llama-3.1-8b`
- `llama-3.2-1b`
- `llama-3.2-3b`
- `qwen-2.5-coder-14b`
- `qwen-2.5-coder-32b`
- `qwen-2.5-coder-7b`
- `qwen-2.5-14b`
- `qwen-2.5-7b`

### DeepSeek (3 ä¸ªæ¨¡å‹)

- `deepseek-chat` - é€šç”¨å¯¹è¯æ¨¡å‹
- `deepseek-coder` - ä»£ç ä¸“ç”¨æ¨¡å‹
- `deepseek-reasoner` - æ·±åº¦æ¨ç†æ¨¡å‹

### OpenAI (4 ä¸ªæ¨¡å‹)

- `gpt-4o` - æœ€æ–°æ——èˆ°æ¨¡å‹
- `gpt-4o-mini` - è½»é‡ç‰ˆæœ¬
- `gpt-4-turbo` - GPT-4 åŠ é€Ÿç‰ˆ
- `gpt-3.5-turbo` - ç»å…¸æ¨¡å‹

### DashScope / Qwen (11 ä¸ªæ¨¡å‹)

- `qwen-max` - æœ€å¼ºæ¨¡å‹
- `qwen-plus` - å¹³è¡¡æ€§èƒ½
- `qwen-turbo` - å¿«é€Ÿå“åº”
- `qwen-3-235b-a22b-thinking-2507` - æ·±åº¦æ€è€ƒæ¨¡å‹
- `qwen-3-350b` - è¶…å¤§å‚æ•°æ¨¡å‹
- `qwen-3-32b` / `qwen-3-14b` / `qwen-3-7b` - ä¸åŒè§„æ¨¡
- `qwen-2.5-coder-32b-instruct` - ä»£ç ä¼˜åŒ–
- ç­‰...

### Kimi / Moonshot (7 ä¸ªæ¨¡å‹)

- `moonshot-v1-8k` - 8K ä¸Šä¸‹æ–‡
- `moonshot-v1-32k` - 32K ä¸Šä¸‹æ–‡
- `moonshot-v1-128k` - 128K ä¸Šä¸‹æ–‡
- `kimi-k2-0905-preview` - K2 é¢„è§ˆç‰ˆï¼ˆ256Kï¼‰
- `kimi-k2-turbo-preview` - K2 é«˜é€Ÿç‰ˆï¼ˆ60-100 Tokens/sï¼‰
- `kimi-k2-thinking` - K2 é•¿æ€è€ƒæ¨¡å‹ï¼ˆ256Kï¼‰
- `kimi-k2-thinking-turbo` - K2 é•¿æ€è€ƒé«˜é€Ÿç‰ˆ

## ğŸ”§ é«˜çº§é…ç½®

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

**ä½¿ç”¨ Gunicorn + Uvicorn Workerï¼š**

```bash
pip install gunicorn

gunicorn src.fastapi_server:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --access-logfile - \
  --error-logfile -
```

**ä½¿ç”¨ Dockerï¼š**

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "src.fastapi_server:app", "--host", "0.0.0.0", "--port", "8000"]
```

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­ï¼š

```env
# API å¯†é’¥
CEREBRAS_API_KEY=your_key
DEEPSEEK_API_KEY=your_key
OPENAI_API_KEY=your_key
DASHSCOPE_API_KEY=your_key
KIMI_API_KEY=your_key

# æœåŠ¡å™¨é…ç½®ï¼ˆå¯é€‰ï¼‰
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000
FASTAPI_RELOAD=True
```

### è‡ªå®šä¹‰ç«¯å£

åœ¨ `fastapi_main.py` ä¸­ä¿®æ”¹ï¼š

```python
config = {
    "host": "0.0.0.0",
    "port": 8080,  # è‡ªå®šä¹‰ç«¯å£
    ...
}
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

| æ¨¡å‹            | æä¾›å•†       | å¹³å‡å»¶è¿Ÿ | ååé‡       | é€‚ç”¨åœºæ™¯      |
|---------------|-----------|------|-----------|-----------|
| llama-3.3-70b | Cerebras  | ~2s  | æé«˜        | å¿«é€Ÿå¯¹è¯ã€å®æ—¶åº”ç”¨ |
| deepseek-chat | DeepSeek  | ~3s  | é«˜         | é€šç”¨å¯¹è¯ã€ä¸­æ–‡ä¼˜åŒ– |
| gpt-4o        | OpenAI    | ~5s  | ä¸­         | å¤æ‚æ¨ç†ã€ä¸“ä¸šè¾“å‡º |
| qwen-max      | DashScope | ~4s  | é«˜         | ä¸­æ–‡ä»»åŠ¡ã€çŸ¥è¯†é—®ç­” |
| kimi-k2-turbo | Kimi      | ~2s  | 60-100T/s | å¿«é€Ÿå“åº”ã€é•¿ä¸Šä¸‹æ–‡ |

## ğŸ”’ å®‰å…¨å»ºè®®

1. **ç”Ÿäº§ç¯å¢ƒåŠ¡å¿…æ·»åŠ è®¤è¯**ï¼š

```python
from fastapi import Security, HTTPException
from fastapi.security import HTTPBearer

security = HTTPBearer()

@app.post("/v1/chat/completions")
async def create_chat_completion(
    request: ChatCompletionRequest,
    credentials: HTTPAuthorizationCredentials = Security(security)
):
    # éªŒè¯ token
    if credentials.credentials != "your-secret-token":
        raise HTTPException(status_code=401, detail="Unauthorized")
    ...
```

2. **é™æµä¿æŠ¤**ï¼š

```bash
pip install slowapi

from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/v1/chat/completions")
@limiter.limit("10/minute")
async def create_chat_completion(...):
    ...
```

3. **HTTPS éƒ¨ç½²**ï¼šä½¿ç”¨ Nginx æˆ– Caddy åå‘ä»£ç†ã€‚

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ‡æ¢é»˜è®¤æ¨¡å‹ï¼Ÿ

åœ¨è¯·æ±‚ä¸­æŒ‡å®š `model` å‚æ•°å³å¯ï¼Œæ— éœ€é…ç½®ã€‚

### Q2: æ”¯æŒå¤šè½®å¯¹è¯å—ï¼Ÿ

æ”¯æŒï¼åœ¨ `messages` æ•°ç»„ä¸­æ·»åŠ å†å²æ¶ˆæ¯ï¼š

```json
{
  "messages": [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"},
    {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—"}
  ]
}
```

### Q3: å¦‚ä½•å¤„ç†é”™è¯¯ï¼Ÿ

æ‰€æœ‰é”™è¯¯è¿”å›ç»Ÿä¸€æ ¼å¼ï¼š

```json
{
  "error": {
    "message": "é”™è¯¯æè¿°",
    "type": "error_type",
    "code": 400
  }
}
```

### Q4: èƒ½å¦ä¸ç°æœ‰ OpenAI ä»£ç æ— ç¼é›†æˆï¼Ÿ

å¯ä»¥ï¼åªéœ€ä¿®æ”¹ `base_url`ï¼š

```python
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)
```

### Q5: æµå¼å“åº”å¦‚ä½•å¤„ç†ï¼Ÿ

ä½¿ç”¨ SSEï¼ˆServer-Sent Eventsï¼‰æ ¼å¼ï¼Œé€è¡Œè¯»å– `data:` å¼€å¤´çš„å†…å®¹ã€‚

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [FastAPI å®˜æ–¹æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [OpenAI API å‚è€ƒ](https://platform.openai.com/docs/api-reference)
- [é¡¹ç›®æ ¹æ–‡æ¡£](../CLAUDE.md)
- [æ·±åº¦æ€è€ƒåŠŸèƒ½](deep_thinking_feature.md)

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-12-09)

- âœ… åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼
- âœ… æ”¯æŒ 5 ä¸ªæä¾›å•†ï¼Œ35+ æ¨¡å‹
- âœ… æ”¯æŒæµå¼å’Œéæµå¼å“åº”
- âœ… æä¾›å®Œæ•´æµ‹è¯•å¥—ä»¶
- âœ… é›†æˆç°æœ‰ MultiProviderAPIService

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚

---

**äº«å—ä½¿ç”¨ ThinkCloud FastAPI æœåŠ¡ï¼** ğŸš€
