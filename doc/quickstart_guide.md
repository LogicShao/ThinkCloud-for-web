# å¿«é€Ÿå¼€å§‹æŒ‡å— - ä½¿ç”¨æ–°åŠŸèƒ½

> ç®€æ˜æ•™ç¨‹,5åˆ†é’Ÿä¸Šæ‰‹å¼‚æ­¥APIã€é”™è¯¯å¤„ç†ã€ç¼“å­˜å’Œä¼šè¯æŒä¹…åŒ–

---

## ğŸ“¦ åŠŸèƒ½æ¦‚è§ˆ

âœ… **å¼‚æ­¥å¤„ç†** - è¯·æ±‚å–æ¶ˆã€è¶…æ—¶æ§åˆ¶ã€å¹¶å‘ä¼˜åŒ–
âœ… **é”™è¯¯å¤„ç†** - æ™ºèƒ½åˆ†ç±»ã€ç”¨æˆ·å‹å¥½ã€è‡ªåŠ¨é‡è¯•å»ºè®®
âœ… **ç¼“å­˜ä¼˜åŒ–** - LRUç­–ç•¥ã€æŒä¹…åŒ–ã€å¤šå±‚ç¼“å­˜
âœ… **ä¼šè¯ç®¡ç†** - è‡ªåŠ¨ä¿å­˜ã€å†å²æ¢å¤ã€å¯¼å…¥å¯¼å‡º

---

## ğŸš€ å¿«é€Ÿç¤ºä¾‹

### ç¤ºä¾‹ 1: å¼‚æ­¥APIè°ƒç”¨

```python
from src.async_api_service import async_api_service
import asyncio

async def chat():
    # ç®€å•è°ƒç”¨
    response = await async_api_service.chat_completion(
        messages=[{"role": "user", "content": "ä½ å¥½"}],
        model="llama-3.3-70b"
    )
    print(response)

# è¿è¡Œ
asyncio.run(chat())
```

**ç‰¹æ€§:**

- â±ï¸ 30ç§’è‡ªåŠ¨è¶…æ—¶
- ğŸ’¾ è‡ªåŠ¨ç¼“å­˜ç»“æœ
- ğŸ”„ æ”¯æŒä¸­é€”å–æ¶ˆ

---

### ç¤ºä¾‹ 2: å¸¦è¶…æ—¶å’Œå–æ¶ˆçš„è°ƒç”¨

```python
async def chat_with_cancel():
    # åˆ›å»ºå–æ¶ˆä»¤ç‰Œ
    request_id = async_api_service.create_cancellation_token()

    # 5ç§’åå–æ¶ˆ(æ¨¡æ‹Ÿç”¨æˆ·å–æ¶ˆæ“ä½œ)
    asyncio.create_task(cancel_later(request_id, 5))

    try:
        response = await async_api_service.chat_completion(
            messages=[{"role": "user", "content": "å†™ä¸€ç¯‡é•¿æ–‡ç« "}],
            model="llama-3.3-70b",
            timeout=10.0,  # 10ç§’è¶…æ—¶
            request_id=request_id
        )
        print(response)
    except Exception as e:
        print(f"è°ƒç”¨è¢«ä¸­æ–­: {e}")

async def cancel_later(request_id, delay):
    await asyncio.sleep(delay)
    async_api_service.cancel_request(request_id)
    print("å·²å–æ¶ˆè¯·æ±‚")

asyncio.run(chat_with_cancel())
```

---

### ç¤ºä¾‹ 3: æ™ºèƒ½é”™è¯¯å¤„ç†

```python
from src.error_handler import error_handler

async def safe_chat():
    try:
        response = await async_api_service.chat_completion(
            messages=[{"role": "user", "content": "ä½ å¥½"}],
            model="gpt-4o"
        )
        return response
    except Exception as e:
        # æ ¼å¼åŒ–é”™è¯¯
        error = error_handler.handle_error(
            error=e,
            provider="openai",
            model="gpt-4o",
            operation="èŠå¤©"
        )

        # æ‰“å°ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        print(error.to_user_message())

        # è‡ªåŠ¨é‡è¯•é€»è¾‘
        if error.context.is_retryable:
            print(f"ç­‰å¾…{error.context.retry_after}ç§’åé‡è¯•...")
            await asyncio.sleep(error.context.retry_after)
            # é‡æ–°è°ƒç”¨...

asyncio.run(safe_chat())
```

**è¾“å‡ºç¤ºä¾‹:**

```
âŒ **é”™è¯¯**: èŠå¤©å¤±è´¥: è¯·æ±‚é¢‘ç‡è¶…å‡ºé™åˆ¶
ğŸ“ **æä¾›å•†**: openai
ğŸ¤– **æ¨¡å‹**: gpt-4o
ğŸ’¡ **å»ºè®®**: è¯·ç¨åé‡è¯•,æˆ–å‡çº§APIå¥—é¤
â±ï¸ **é‡è¯•**: è¯·ç­‰å¾… 60 ç§’åé‡è¯•
```

---

### ç¤ºä¾‹ 4: ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ

```python
from src.cache_manager import response_cache, generate_cache_key
from datetime import timedelta

# ç”Ÿæˆç¼“å­˜é”®
cache_key = generate_cache_key(
    "chat",
    model="llama-3.3-70b",
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    temperature=0.7
)

# æ£€æŸ¥ç¼“å­˜
cached_response = response_cache.get(cache_key)
if cached_response:
    print("ä½¿ç”¨ç¼“å­˜å“åº”")
    print(cached_response)
else:
    # è°ƒç”¨API
    response = await async_api_service.chat_completion(...)

    # ç¼“å­˜ç»“æœ
    response_cache.set(cache_key, response, ttl=timedelta(minutes=10))
```

---

### ç¤ºä¾‹ 5: ä¼šè¯æŒä¹…åŒ–

```python
from src.session_store import session_store, ModelConfig

# åˆ›å»ºä¼šè¯
session = session_store.create_session()

# æ·»åŠ å¯¹è¯
session_store.update_chat_history("user", "ä»‹ç»ä¸€ä¸‹Python")
session_store.update_chat_history("assistant", "Pythonæ˜¯ä¸€é—¨...")

# ä¿å­˜æ¨¡å‹é…ç½®
config = ModelConfig(
    provider="openai",
    model="gpt-4o",
    temperature=0.8
)
session_store.update_model_config(config)

# è‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜
session_store.save_session(session)

# ä¸‹æ¬¡å¯åŠ¨æ—¶åŠ è½½
loaded = session_store.load_session(session.session_id)
print(f"æ¢å¤äº†{len(loaded.chat_history)}æ¡å¯¹è¯å†å²")
```

---

### ç¤ºä¾‹ 6: å¼‚æ­¥æ·±åº¦æ€è€ƒ

```python
from src.async_api_service import async_api_service
from src.async_deep_think import AsyncDeepThinkOrchestrator

async def deep_think():
    orchestrator = AsyncDeepThinkOrchestrator(
        async_api_service=async_api_service,
        model="qwen-3-235b-a22b-thinking-2507",
        max_subtasks=6,
        max_parallel_tasks=3,  # 3ä¸ªå­ä»»åŠ¡å¹¶è¡Œ
        enable_review=True,
        verbose=True
    )

    result = await orchestrator.run("å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½?")

    print(f"LLMè°ƒç”¨: {result.total_llm_calls} æ¬¡")
    print(f"ç­”æ¡ˆ: {result.final_answer}")

asyncio.run(deep_think())
```

**æ€§èƒ½å¯¹æ¯”:**

- ä¸²è¡Œæ‰§è¡Œ: ~120ç§’
- å¹¶è¡Œæ‰§è¡Œ: ~60ç§’ (æå‡50%)

---

## ğŸ› ï¸ å¸¸ç”¨ä»£ç ç‰‡æ®µ

### 1. æ‰¹é‡APIè°ƒç”¨

```python
async def batch_chat(questions):
    tasks = []
    for q in questions:
        task = async_api_service.chat_completion(
            messages=[{"role": "user", "content": q}],
            model="llama-3.3-70b"
        )
        tasks.append(task)

    # å¹¶å‘æ‰§è¡Œ
    responses = await asyncio.gather(*tasks)
    return responses

questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
results = asyncio.run(batch_chat(questions))
```

---

### 2. å¸¦é‡è¯•çš„APIè°ƒç”¨

```python
from src.error_handler import error_handler

async def call_with_retry(max_retries=3):
    for i in range(max_retries):
        try:
            return await async_api_service.chat_completion(...)
        except Exception as e:
            error = error_handler.handle_error(e)

            if not error.context.is_retryable or i == max_retries - 1:
                raise

            print(f"é‡è¯• {i+1}/{max_retries}...")
            await asyncio.sleep(error.context.retry_after or 5)
```

---

### 3. ç¼“å­˜è£…é¥°å™¨

```python
from src.cache_manager import response_cache, generate_cache_key
from functools import wraps

def cached_api_call(ttl_minutes=10):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            key = generate_cache_key(func.__name__, **kwargs)

            # æ£€æŸ¥ç¼“å­˜
            cached = response_cache.get(key)
            if cached:
                return cached

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)

            # ç¼“å­˜ç»“æœ
            response_cache.set(key, result, ttl=timedelta(minutes=ttl_minutes))
            return result

        return wrapper
    return decorator

@cached_api_call(ttl_minutes=15)
async def my_api_call(model, messages):
    return await async_api_service.chat_completion(
        messages=messages,
        model=model
    )
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. åˆç†è®¾ç½®å¹¶å‘æ•°

```python
# æ ¹æ®APIé™æµè°ƒæ•´
async_api_service = AsyncAPIService(
    max_concurrent_requests=10  # OpenAI: 10-20, Cerebras: 5-10
)
```

---

### 2. å¯ç”¨ç¼“å­˜

```python
# é«˜å‘½ä¸­ç‡åœºæ™¯(é‡å¤æŸ¥è¯¢å¤š)
response = await async_api_service.chat_completion(
    ...,
    enable_cache=True  # é»˜è®¤å¼€å¯
)

# ä½å‘½ä¸­ç‡åœºæ™¯(æ¯æ¬¡æŸ¥è¯¢ä¸åŒ)
response = await async_api_service.chat_completion(
    ...,
    enable_cache=False  # èŠ‚çœå†…å­˜
)
```

---

### 3. å®šæœŸæ¸…ç†ç¼“å­˜

```python
from src.cache_manager import response_cache

# æ¸…ç†è¿‡æœŸæ¡ç›®
expired_count = response_cache.cleanup_expired()
print(f"æ¸…ç†äº†{expired_count}ä¸ªè¿‡æœŸæ¡ç›®")

# æŸ¥çœ‹ç»Ÿè®¡
stats = response_cache.get_stats()
print(f"å‘½ä¸­ç‡: {stats['hit_rate']}")
```

---

## âš™ï¸ é…ç½®å»ºè®®

### å¼€å‘ç¯å¢ƒ

```python
# å¼‚æ­¥API
AsyncAPIService(max_concurrent_requests=5)

# ç¼“å­˜
CacheManager(
    max_size=100,
    max_memory_mb=10,
    default_ttl=timedelta(minutes=5),
    enable_persistence=False  # å¼€å‘ç¯å¢ƒå¯å…³é—­æŒä¹…åŒ–
)

# æ·±åº¦æ€è€ƒ
AsyncDeepThinkOrchestrator(
    max_parallel_tasks=2,
    enable_review=False,  # å¿«é€Ÿæµ‹è¯•
    verbose=True
)
```

---

### ç”Ÿäº§ç¯å¢ƒ

```python
# å¼‚æ­¥API
AsyncAPIService(max_concurrent_requests=20)

# ç¼“å­˜
CacheManager(
    max_size=1000,
    max_memory_mb=100,
    default_ttl=timedelta(minutes=10),
    enable_persistence=True  # æŒä¹…åŒ–
)

# æ·±åº¦æ€è€ƒ
AsyncDeepThinkOrchestrator(
    max_parallel_tasks=3,
    enable_review=True,
    verbose=False  # å‡å°‘æ—¥å¿—
)
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: "No module named 'src'"

**è§£å†³:**

```python
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
```

---

### é—®é¢˜ 2: "This event loop is already running"

**è§£å†³:**

```python
# ä¸è¦åœ¨Jupyter/IPythonä¸­ä½¿ç”¨asyncio.run()
# ç›´æ¥ä½¿ç”¨await

# é”™è¯¯
# asyncio.run(main())

# æ­£ç¡® (Jupyter)
await main()

# æ­£ç¡® (æ™®é€šè„šæœ¬)
if __name__ == "__main__":
    asyncio.run(main())
```

---

### é—®é¢˜ 3: ç¼“å­˜æœªå‘½ä¸­

**æ£€æŸ¥:**

```python
from src.cache_manager import response_cache

# æŸ¥çœ‹ç»Ÿè®¡
stats = response_cache.get_stats()
print(stats)

# ç¡®è®¤ç¼“å­˜é”®ä¸€è‡´
key1 = generate_cache_key("api", model="gpt-4o", temp=0.7)
key2 = generate_cache_key("api", model="gpt-4o", temp=0.7)
assert key1 == key2  # åº”è¯¥ç›¸ç­‰
```

---

## ğŸ“š æ›´å¤šèµ„æº

- **å®Œæ•´æ–‡æ¡£**: `doc/advanced_features_improvement.md`
- **æµ‹è¯•ç¤ºä¾‹**: `tests/test_new_features.py`
- **æºä»£ç **: `src/async_api_service.py`, `src/error_handler.py` ç­‰

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«! ğŸ‰**
