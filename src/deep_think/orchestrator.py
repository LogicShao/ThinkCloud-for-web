"""
æ·±åº¦æ€è€ƒç¼–æŽ’å™¨
ç®¡ç†å¤šé˜¶æ®µæŽ¨ç†æµç¨‹ï¼Œåè°ƒå„ä¸ªé˜¶æ®µå¤„ç†å™¨
"""

from typing import Optional

from src.logging import (
    EnhancedLogger,
    LogContext,
    get_deep_think_orchestrator_logger,
    log_function_call,
)

from .core.interfaces import ILLMService, IOrchestrator
from .core.models import (
    DeepThinkResult,
    Plan,
    ReviewResult,
    StageContext,
    SubtaskResult,
    ThinkingStage,
)
from .prompts.manager import PromptTemplateManager
from .stages import (
    PlannerStageProcessor,
    ReviewerStageProcessor,
    SolverStageProcessor,
    SynthesizerStageProcessor,
)
from .utils import DefaultJSONParser, MemoryCacheManager, generate_cache_key


class DeepThinkOrchestrator(IOrchestrator):
    """æ·±åº¦æ€è€ƒç¼–æŽ’å™¨ - ç®¡ç†å¤šé˜¶æ®µæŽ¨ç†æµç¨‹"""

    def __init__(
        self,
        api_service: ILLMService,
        model: str,
        max_subtasks: int = 6,
        enable_review: bool = True,
        enable_web_search: bool = False,
        verbose: bool = True,
        system_instruction: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        request_id: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–æ·±åº¦æ€è€ƒç¼–æŽ’å™¨

        Args:
            api_service: MultiProviderAPIServiceå®žä¾‹
            model: ä½¿ç”¨çš„æ¨¡åž‹åç§°
            max_subtasks: æœ€å¤§å­ä»»åŠ¡æ•°é‡
            enable_review: æ˜¯å¦å¯ç”¨æœ€ç»ˆå®¡æŸ¥
            enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢åŠŸèƒ½
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            system_instruction: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            top_p: æ ¸é‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            request_id: è¯·æ±‚IDï¼Œç”¨äºŽæ—¥å¿—è¿½è¸ª
        """
        self.api_service = api_service
        self.model = model
        self.max_subtasks = max_subtasks
        self.enable_review = enable_review
        self.enable_web_search = enable_web_search
        self.verbose = verbose

        # æ¨¡åž‹å‚æ•°
        self.system_instruction = system_instruction
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens

        # åˆå§‹åŒ–Webæœç´¢å·¥å…·
        self.web_search_tool = None
        if enable_web_search:
            try:
                from src.tools.web_search import WebSearchTool

                self.web_search_tool = WebSearchTool()
                if not self.web_search_tool.is_available():
                    print("[WARN] Webæœç´¢å·¥å…·ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install duckduckgo-search")
                    self.web_search_tool = None
            except Exception as e:
                print(f"[ERROR] åˆå§‹åŒ–Webæœç´¢å·¥å…·å¤±è´¥: {e}")
                self.web_search_tool = None

        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self.logger = get_deep_think_orchestrator_logger(
            LogContext(
                request_id=request_id,
                module="orchestrator",
                custom_fields={
                    "model": model,
                    "max_subtasks": max_subtasks,
                    "enable_review": enable_review,
                    "verbose": verbose,
                },
            )
        )

        # åˆå§‹åŒ–ç»„ä»¶
        self.json_parser = DefaultJSONParser()
        self.cache_manager = MemoryCacheManager()
        self.prompt_manager = PromptTemplateManager()

        # åˆå§‹åŒ–é˜¶æ®µå¤„ç†å™¨
        self._initialize_stage_processors()

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_llm_calls = 0

        self.logger.info("ç¼–æŽ’å™¨åˆå§‹åŒ–å®Œæˆ")

    def _initialize_stage_processors(self):
        """åˆå§‹åŒ–é˜¶æ®µå¤„ç†å™¨"""
        # èŽ·å–å„ä¸ªé˜¶æ®µçš„æ¨¡æ¿
        plan_template = self.prompt_manager.get_template_by_stage(ThinkingStage.PLAN)
        solve_template = self.prompt_manager.get_template_by_stage(ThinkingStage.SOLVE)
        synthesize_template = self.prompt_manager.get_template_by_stage(ThinkingStage.SYNTHESIZE)
        review_template = self.prompt_manager.get_template_by_stage(ThinkingStage.REVIEW)

        # åˆ›å»ºé˜¶æ®µå¤„ç†å™¨
        self.planner = PlannerStageProcessor(
            llm_service=self.api_service,
            json_parser=self.json_parser,
            prompt_template=plan_template,
            max_subtasks=self.max_subtasks,
            verbose=self.verbose,
        )

        self.solver = SolverStageProcessor(
            llm_service=self.api_service,
            json_parser=self.json_parser,
            prompt_template=solve_template,
            verbose=self.verbose,
            web_search_tool=self.web_search_tool if self.enable_web_search else None,
        )

        self.synthesizer = SynthesizerStageProcessor(
            llm_service=self.api_service,
            json_parser=self.json_parser,
            prompt_template=synthesize_template,
            verbose=self.verbose,
        )

        self.reviewer = ReviewerStageProcessor(
            llm_service=self.api_service,
            json_parser=self.json_parser,
            prompt_template=review_template,
            verbose=self.verbose,
        )

    @log_function_call(level=5)  # TRACEçº§åˆ«
    def run(self, question: str, **kwargs) -> DeepThinkResult:
        """
        æ‰§è¡Œæ·±åº¦æ€è€ƒæµç¨‹

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            DeepThinkResult: å®Œæ•´çš„æ€è€ƒç»“æžœ
        """
        self.logger.info("å¼€å§‹æ·±åº¦æ€è€ƒæµç¨‹", question_preview=question[:50])

        try:
            # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            context = self._create_context()

            # é˜¶æ®µ1: è§„åˆ’
            with self.logger.timer("plan_stage"):
                plan = self._execute_plan_stage(context, question)

            # é˜¶æ®µ2: é€ä¸ªè§£å†³å­ä»»åŠ¡
            with self.logger.timer("solve_stage"):
                subtask_results = self._execute_solve_stage(context, question, plan)

            # é˜¶æ®µ3: æ•´åˆç»“æžœ
            with self.logger.timer("synthesize_stage"):
                final_answer = self._execute_synthesize_stage(
                    context, question, plan, subtask_results
                )

            # é˜¶æ®µ4: å¯é€‰å®¡æŸ¥
            review_result = None
            if self.enable_review:
                with self.logger.timer("review_stage"):
                    review_result = self._execute_review_stage(context, question, final_answer)

            # ç”Ÿæˆæ€è€ƒè¿‡ç¨‹æ‘˜è¦
            thinking_summary = self._generate_thinking_summary(plan, subtask_results)

            # æ›´æ–°æ€»LLMè°ƒç”¨æ¬¡æ•°
            self.total_llm_calls = context.llm_call_count

            result = DeepThinkResult(
                original_question=question,
                final_answer=final_answer,
                plan=plan,
                subtask_results=subtask_results,
                review=review_result,
                total_llm_calls=self.total_llm_calls,
                thinking_process_summary=thinking_summary,
            )

            self.logger.info(
                "æ·±åº¦æ€è€ƒæµç¨‹å®Œæˆ",
                total_llm_calls=self.total_llm_calls,
                subtask_count=len(subtask_results),
                has_review=review_result is not None,
                final_answer_length=len(final_answer),
            )

            # è®°å½•æ€§èƒ½æ•°æ®
            self._log_performance_summary(context)

            return result

        except Exception as e:
            self.logger.log_exception("æ·±åº¦æ€è€ƒæµç¨‹æ‰§è¡Œå¤±è´¥", e)
            # è¿”å›žä¸€ä¸ªé”™è¯¯ç»“æžœ
            return self._create_error_result(question, e)

    def _create_context(self) -> StageContext:
        """åˆ›å»ºé˜¶æ®µæ‰§è¡Œä¸Šä¸‹æ–‡"""
        return StageContext(
            original_question="",  # å°†åœ¨å„é˜¶æ®µè®¾ç½®
            model=self.model,
            system_instruction=self.system_instruction,
            temperature=self.temperature,
            top_p=self.top_p,
            max_tokens=self.max_tokens,
            verbose=self.verbose,
            llm_call_count=0,
        )

    def _execute_plan_stage(self, context: StageContext, question: str) -> Plan:
        """æ‰§è¡Œè§„åˆ’é˜¶æ®µ"""
        # å°è¯•ä»Žç¼“å­˜èŽ·å–
        cache_key = generate_cache_key("plan", question)
        cached_result = self.cache_manager.get(cache_key)
        if cached_result is not None:
            if self.verbose:
                self.logger.debug("ä»Žç¼“å­˜èŽ·å–è§„åˆ’")
            return cached_result

        # æ‰§è¡Œè§„åˆ’
        result = self.planner.execute(
            context,
            question=question,
        )

        if not result.success:
            raise RuntimeError(f"è§„åˆ’é˜¶æ®µå¤±è´¥: {result.error}")

        plan = result.data
        context.llm_call_count += result.llm_calls

        # å­˜å‚¨åˆ°ç¼“å­˜
        self.cache_manager.set(cache_key, plan)
        return plan

    def _execute_solve_stage(
        self, context: StageContext, question: str, plan: Plan
    ) -> list[SubtaskResult]:
        """æ‰§è¡Œè§£å†³é˜¶æ®µ"""
        subtask_results = []
        for subtask in plan.subtasks:
            result = self.solver.execute(
                context,
                subtask=subtask,
                original_question=question,
                previous_results=subtask_results,
            )

            if not result.success:
                self.logger.warning("å­ä»»åŠ¡æ‰§è¡Œå¤±è´¥", subtask_id=subtask.id, error=result.error)
                # ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªå­ä»»åŠ¡

            subtask_results.append(result.data)
            context.llm_call_count += result.llm_calls

        return subtask_results

    def _execute_synthesize_stage(
        self,
        context: StageContext,
        question: str,
        plan: Plan,
        subtask_results: list[SubtaskResult],
    ) -> str:
        """æ‰§è¡Œæ•´åˆé˜¶æ®µ"""
        result = self.synthesizer.execute(
            context,
            original_question=question,
            plan=plan,
            subtask_results=subtask_results,
        )

        if not result.success:
            raise RuntimeError(f"æ•´åˆé˜¶æ®µå¤±è´¥: {result.error}")

        context.llm_call_count += result.llm_calls
        return result.data

    def _execute_review_stage(
        self, context: StageContext, question: str, final_answer: str
    ) -> ReviewResult:
        """æ‰§è¡Œå®¡æŸ¥é˜¶æ®µ"""
        result = self.reviewer.execute(
            context,
            original_question=question,
            final_answer=final_answer,
        )

        if not result.success:
            self.logger.warning("å®¡æŸ¥é˜¶æ®µå¤±è´¥", error=result.error)
            # è¿”å›žé»˜è®¤å®¡æŸ¥ç»“æžœ
            return ReviewResult(
                issues_found=[],
                improvement_suggestions=[],
                overall_quality_score=0.7,
                review_notes="å®¡æŸ¥é˜¶æ®µæ‰§è¡Œå¤±è´¥",
            )

        context.llm_call_count += result.llm_calls
        return result.data

    def _generate_thinking_summary(self, plan: Plan, subtask_results: list[SubtaskResult]) -> str:
        """ç”Ÿæˆæ€è€ƒè¿‡ç¨‹æ‘˜è¦"""
        summary_parts = [
            "## ðŸ§  æ·±åº¦æ€è€ƒè¿‡ç¨‹æ‘˜è¦\n",
            f"**é—®é¢˜æ¾„æ¸…:** {plan.clarified_question}\n",
            f"**æŽ¨ç†ç­–ç•¥:** {plan.reasoning_approach}\n",
            "\n**å­ä»»åŠ¡æ‰§è¡Œæƒ…å†µ:**",
        ]

        for result in subtask_results:
            # ç§»é™¤æˆªæ–­é™åˆ¶ï¼Œæ˜¾ç¤ºå®Œæ•´ç»“è®º
            conclusion_display = result.intermediate_conclusion

            summary_parts.append(
                f"\n{result.subtask_id}. {result.description}\n"
                f"   - å¯ä¿¡åº¦: {result.confidence:.0%}\n"
                f"   - ç»“è®º: {conclusion_display}"
            )

        return "\n".join(summary_parts)

    def _create_error_result(self, question: str, error: Exception) -> DeepThinkResult:
        """åˆ›å»ºé”™è¯¯ç»“æžœ"""
        return DeepThinkResult(
            original_question=question,
            final_answer=f"æ·±åº¦æ€è€ƒè¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {error!s}",
            plan=Plan(clarified_question=question, subtasks=[], plan_text=""),
            subtask_results=[],
            total_llm_calls=self.total_llm_calls,
        )

    def _log_performance_summary(self, context: StageContext) -> None:
        """è®°å½•æ€§èƒ½æ‘˜è¦"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ€§èƒ½æŒ‡æ ‡
        self.logger.debug(
            "æ€§èƒ½æ‘˜è¦",
            total_llm_calls=context.llm_call_count,
            model=self.model,
            max_subtasks=self.max_subtasks,
            enable_review=self.enable_review,
        )

    def get_cache_stats(self) -> dict:
        """èŽ·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "size": self.cache_manager.size(),
        }

    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.cache_manager.clear()
        self.logger.info("ç¼“å­˜å·²æ¸…ç©º")
