"""
å“åº”å¤„ç†å™¨ - å¤„ç†LLMå“åº”é€»è¾‘
åŒ…å«æ ‡å‡†æ¨¡å¼å“åº”å¤„ç†å’Œæ·±åº¦æ€è€ƒæ¨¡å¼å“åº”å¤„ç†
"""

from datetime import datetime
from typing import List, Dict, Any, Optional

from src.api_service import api_service
from src.chat_manager import ChatManager
from src.deep_think import DeepThinkOrchestrator, format_deep_think_result


class ResponseHandler:
    """æ ‡å‡†å“åº”å¤„ç†å™¨"""

    def __init__(self, chat_manager: ChatManager):
        """
        åˆå§‹åŒ–å“åº”å¤„ç†å™¨

        Args:
            chat_manager: èŠå¤©ç®¡ç†å™¨å®ä¾‹
        """
        self.chat_manager = chat_manager

    def handle_standard_response(
            self,
            history: List[Dict[str, Any]],
            model: str,
            enable_stream: bool,
            start_time: datetime,
            system_instruction: Optional[str] = None,
            temperature: Optional[float] = None,
            top_p: Optional[float] = None,
            max_tokens: Optional[int] = None,
            frequency_penalty: Optional[float] = None,
            presence_penalty: Optional[float] = None,
    ):
        """
        å¤„ç†æ ‡å‡†æ¨¡å¼å“åº”ï¼ˆæµå¼æˆ–éæµå¼ï¼‰

        Args:
            history: å¯¹è¯å†å²
            model: æ¨¡å‹åç§°
            enable_stream: æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“
            start_time: å¼€å§‹æ—¶é—´
            system_instruction: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top På‚æ•°
            max_tokens: æœ€å¤§Tokenæ•°
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            presence_penalty: å­˜åœ¨æƒ©ç½š

        Yields:
            List[Dict]: æ›´æ–°åçš„å¯¹è¯å†å²
        """
        # æ„å»ºAPIæ¶ˆæ¯
        api_messages = []
        for msg in history:
            if msg["role"] in ["user", "assistant"]:
                api_messages.append({"role": msg["role"], "content": msg["content"]})

        time_str = start_time.strftime("%H:%M:%S")

        if enable_stream:
            # æµå¼ä¼ è¾“æ¨¡å¼
            yield from self._handle_streaming_response(
                history=history,
                api_messages=api_messages,
                model=model,
                start_time=start_time,
                time_str=time_str,
                system_instruction=system_instruction,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
            )
        else:
            # éæµå¼ä¼ è¾“æ¨¡å¼
            yield from self._handle_non_streaming_response(
                history=history,
                api_messages=api_messages,
                model=model,
                start_time=start_time,
                time_str=time_str,
                system_instruction=system_instruction,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
            )

    def _handle_streaming_response(
            self,
            history: List[Dict[str, Any]],
            api_messages: List[Dict[str, str]],
            model: str,
            start_time: datetime,
            time_str: str,
            system_instruction: Optional[str] = None,
            temperature: Optional[float] = None,
            top_p: Optional[float] = None,
            max_tokens: Optional[int] = None,
            frequency_penalty: Optional[float] = None,
            presence_penalty: Optional[float] = None,
    ):
        """å¤„ç†æµå¼ä¼ è¾“å“åº”"""
        # å…ˆæ·»åŠ ä¸€ä¸ªç©ºçš„åŠ©æ‰‹æ¶ˆæ¯
        history.append(
            {
                "role": "assistant",
                "content": "",
                "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"},
            }
        )

        response_text = ""
        try:
            # è°ƒç”¨APIï¼Œå¯ç”¨æµå¼ä¼ è¾“
            stream_generator = api_service.chat_completion(
                messages=api_messages,
                model=model,
                system_instruction=system_instruction,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens if max_tokens else None,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stream=True,
            )

            # é€æ­¥æ›´æ–°å›å¤
            for chunk in stream_generator:
                response_text += chunk
                # æ›´æ–°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
                history[-1]["content"] = response_text
                yield history

            # æµå¼ä¼ è¾“å®Œæˆï¼Œæ·»åŠ å“åº”æ—¶é—´
            response_text = self._add_duration_to_response(response_text, start_time)
            history[-1]["content"] = response_text
            yield history

        except Exception as e:
            error_msg = f"æµå¼ä¼ è¾“å¤±è´¥: {e!s}"
            error_msg = self._add_duration_to_response(error_msg, start_time)
            history[-1]["content"] = error_msg
            response_text = error_msg
            yield history

        # æ·»åŠ å®Œæ•´å›å¤åˆ°èŠå¤©å†å²ç®¡ç†å™¨
        self.chat_manager.add_message("assistant", response_text)

    def _handle_non_streaming_response(
            self,
            history: List[Dict[str, Any]],
            api_messages: List[Dict[str, str]],
            model: str,
            start_time: datetime,
            time_str: str,
            system_instruction: Optional[str] = None,
            temperature: Optional[float] = None,
            top_p: Optional[float] = None,
            max_tokens: Optional[int] = None,
            frequency_penalty: Optional[float] = None,
            presence_penalty: Optional[float] = None,
    ):
        """å¤„ç†éæµå¼ä¼ è¾“å“åº”"""
        try:
            # è°ƒç”¨API
            response = api_service.chat_completion(
                messages=api_messages,
                model=model,
                system_instruction=system_instruction,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens if max_tokens else None,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stream=False,
            )
        except Exception as e:
            response = f"APIè°ƒç”¨å¤±è´¥: {e!s}"

        # æ·»åŠ å“åº”æ—¶é—´
        response = self._add_duration_to_response(response, start_time)

        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        self.chat_manager.add_message("assistant", response)

        # æ›´æ–°Gradioç•Œé¢
        history.append(
            {
                "role": "assistant",
                "content": response,
                "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"},
            }
        )
        yield history

    @staticmethod
    def _add_duration_to_response(response: str, start_time: datetime) -> str:
        """åœ¨å›å¤å†…å®¹åº•éƒ¨æ·»åŠ å“åº”æ—¶é—´"""
        from datetime import datetime as dt

        end_time = dt.now()
        duration = (end_time - start_time).total_seconds()
        duration_str = ResponseHandler._format_duration(duration)
        return f"{response}\n\n---\nâ±ï¸ **å“åº”æ—¶é—´:** {duration_str}"

    @staticmethod
    def _format_duration(duration_seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´å·®"""
        if duration_seconds < 1:
            return f"{duration_seconds:.2f}s"
        elif duration_seconds < 60:
            return f"{duration_seconds:.1f}s"
        else:
            minutes = int(duration_seconds // 60)
            seconds = int(duration_seconds % 60)
            return f"{minutes}m {seconds}s"


class DeepThinkHandler:
    """æ·±åº¦æ€è€ƒå“åº”å¤„ç†å™¨"""

    def __init__(self, chat_manager: ChatManager):
        """
        åˆå§‹åŒ–æ·±åº¦æ€è€ƒå“åº”å¤„ç†å™¨

        Args:
            chat_manager: èŠå¤©ç®¡ç†å™¨å®ä¾‹
        """
        self.chat_manager = chat_manager

    def handle_deep_think_response(
            self,
            history: List[Dict[str, Any]],
            model: str,
            last_user_msg: str,
            start_time: datetime,
            enable_review: bool,
            show_process: bool,
            max_tasks: int,
            time_str: str,
            system_instruction: Optional[str] = None,
            temperature: Optional[float] = None,
            top_p: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ):
        """
        å¤„ç†æ·±åº¦æ€è€ƒæ¨¡å¼å“åº”

        Args:
            history: å¯¹è¯å†å²
            model: æ¨¡å‹åç§°
            last_user_msg: æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            start_time: å¼€å§‹æ—¶é—´
            enable_review: æ˜¯å¦å¯ç”¨å®¡æŸ¥
            show_process: æ˜¯å¦æ˜¾ç¤ºè¿‡ç¨‹
            max_tasks: æœ€å¤§å­ä»»åŠ¡æ•°
            time_str: æ—¶é—´å­—ç¬¦ä¸²
            system_instruction: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top På‚æ•°
            max_tokens: æœ€å¤§Tokenæ•°

        Yields:
            List[Dict]: æ›´æ–°åçš„å¯¹è¯å†å²
        """
        try:
            orchestrator = DeepThinkOrchestrator(
                api_service=api_service,
                model=model,
                max_subtasks=int(max_tasks),
                enable_review=enable_review,
                verbose=True,
                system_instruction=system_instruction,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens if max_tokens else None,
            )

            result = orchestrator.run(last_user_msg)

            # æ ¼å¼åŒ–ç»“æœ
            response = format_deep_think_result(result, include_process=show_process)

        except Exception as e:
            response = f"æ·±åº¦æ€è€ƒæ¨¡å¼æ‰§è¡Œå¤±è´¥: {e!s}\n\nè¯·å°è¯•å…³é—­æ·±åº¦æ€è€ƒæ¨¡å¼æˆ–æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚"

        # æ·»åŠ å“åº”æ—¶é—´
        response = ResponseHandler._add_duration_to_response(response, start_time)

        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        self.chat_manager.add_message("assistant", response)

        # æ›´æ–°Gradioç•Œé¢ï¼ˆéæµå¼ï¼‰
        history.append(
            {
                "role": "assistant",
                "content": response,
                "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"},
            }
        )
        yield history
