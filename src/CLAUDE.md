# src/ æ¨¡å—æ–‡æ¡£

[æ ¹ç›®å½•](../CLAUDE.md) > **src**

> SimpleLLMFront æ ¸å¿ƒæºä»£ç æ¨¡å—

---

## æ¨¡å—èŒè´£

`src/` ç›®å½•åŒ…å«é¡¹ç›®çš„æ‰€æœ‰æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼š

- **é…ç½®ç®¡ç†**ï¼šç¯å¢ƒå˜é‡ã€æä¾›å•†/æ¨¡å‹æ˜ å°„ã€ç«¯å£å·¥å…·
- **API ç¼–æ’**ï¼šå¤šæä¾›å•†ç»Ÿä¸€æ¥å£ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
- **æä¾›å•†å®ç°**ï¼šå„ AI æä¾›å•†çš„å…·ä½“å®ç°ï¼ˆå·¥å‚æ¨¡å¼ï¼‰
- **å¯¹è¯ç®¡ç†**ï¼šå†å²è®°å½•ã€æ¶ˆæ¯æ ¼å¼è½¬æ¢
- **æ·±åº¦æ€è€ƒ**ï¼šå¤šé˜¶æ®µæ¨ç†ç¼–æ’å™¨

## æ¨¡å—æ¦‚è§ˆ

### ä¾èµ–å…³ç³»å›¾

```
config.py (é…ç½®å±‚)
    â†“
providers.py (æä¾›å•†å±‚)
    â†“
api_service.py (æœåŠ¡å±‚ - å•ä¾‹)
    â†“
deep_think.py (åº”ç”¨å±‚)
    â†“
chat_manager.py (è¾…åŠ©å±‚)
```

### æ¨¡å—æ¸…å•

| æ–‡ä»¶                | è¡Œæ•°  | èŒè´£     | å…³é”®ç±»/å‡½æ•°                                     |
|-------------------|-----|--------|--------------------------------------------|
| `__init__.py`     | 1   | åŒ…æ ‡è¯†    | -                                          |
| `config.py`       | 364 | é…ç½®ç®¡ç†   | `PROVIDER_CONFIG`, `get_server_port()`     |
| `providers.py`    | 476 | æä¾›å•†å®ç°  | `BaseProvider`, `ProviderFactory`          |
| `api_service.py`  | 152 | API ç¼–æ’ | `MultiProviderAPIService` (å•ä¾‹)             |
| `chat_manager.py` | 84  | å¯¹è¯ç®¡ç†   | `ChatManager`, `MessageProcessor`          |
| `deep_think.py`   | 567 | æ·±åº¦æ€è€ƒ   | `DeepThinkOrchestrator`, `PromptTemplates` |

---

## config.py

### æ¨¡å—èŒè´£

- åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆ`.env`ï¼‰
- ç®¡ç†æä¾›å•†é…ç½®å’Œæ¨¡å‹åˆ—è¡¨
- æä¾›ç«¯å£æ£€æµ‹å’Œè‡ªåŠ¨æŸ¥æ‰¾åŠŸèƒ½
- å®šä¹‰æ¨¡å‹å‚æ•°é»˜è®¤å€¼

### å…³é”®é…ç½®

#### æä¾›å•†é…ç½®

```python
PROVIDER_CONFIG = {
    "cerebras": {
        "api_key": os.environ.get("CEREBRAS_API_KEY"),
        "base_url": "https://api.cerebras.ai",
        "enabled": True
    },
    # ... å…¶ä»–æä¾›å•†
}
```

#### æ¨¡å‹æ˜ å°„

```python
PROVIDER_MODELS = {
    "cerebras": ["llama-3.3-70b", ...],  # 10 ä¸ªæ¨¡å‹
    "deepseek": ["deepseek-chat", ...],  # 3 ä¸ªæ¨¡å‹
    "openai": ["gpt-4o", ...],           # 4 ä¸ªæ¨¡å‹
    "dashscope": ["qwen-max", ...],      # 11 ä¸ªæ¨¡å‹
    "kimi": ["moonshot-v1-8k", "kimi-k2-0905-preview", ...],  # 7 ä¸ªæ¨¡å‹
}
```

### å…³é”®å‡½æ•°

#### ç«¯å£ç®¡ç†

```python
def is_port_available(port, host="0.0.0.0") -> bool:
    """æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨"""

def find_available_port(start_port=7860, max_attempts=100, host="0.0.0.0") -> int:
    """ä»èµ·å§‹ç«¯å£å¼€å§‹æŸ¥æ‰¾å¯ç”¨ç«¯å£"""

def get_server_port(preferred_port=SERVER_PORT, host=SERVER_HOST) -> int:
    """è·å–æœåŠ¡å™¨ç«¯å£ï¼ˆä¸»å…¥å£ï¼‰"""
```

#### æä¾›å•†å·¥å…·

```python
def get_enabled_providers() -> List[str]:
    """è·å–å·²é…ç½®ä¸”å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""

def get_model_provider(model: str) -> str:
    """æ ¹æ®æ¨¡å‹åç§°åå‘æŸ¥æ‰¾æä¾›å•†"""

def check_api_key(provider=None) -> bool:
    """æ£€æŸ¥ API å¯†é’¥æ˜¯å¦é…ç½®"""
```

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.config import (
    PROVIDER_CONFIG,
    PROVIDER_MODELS,
    get_server_port,
    get_model_provider
)

# è·å–å¯ç”¨ç«¯å£
port = get_server_port(7860, "0.0.0.0")

# æŸ¥æ‰¾æ¨¡å‹å¯¹åº”çš„æä¾›å•†
provider = get_model_provider("llama-3.3-70b")  # "cerebras"

# è·å–æä¾›å•†é…ç½®
config = PROVIDER_CONFIG["cerebras"]
```

### æ³¨æ„äº‹é¡¹

- ä½¿ç”¨ `python-dotenv` åŠ è½½ç¯å¢ƒå˜é‡
- ç«¯å£æ£€æµ‹ä½¿ç”¨ socket ç»‘å®šæµ‹è¯•
- æ¨¡å‹-æä¾›å•†æ˜ å°„æ˜¯åŒå‘çš„ï¼ˆæ­£å‘å’Œåå‘æŸ¥æ‰¾ï¼‰

---

## providers.py

### æ¨¡å—èŒè´£

- å®šä¹‰æä¾›å•†æŠ½è±¡åŸºç±» `BaseProvider`
- å®ç° 4 ä¸ªå…·ä½“æä¾›å•†ï¼ˆCerebrasã€DeepSeekã€OpenAIã€DashScopeï¼‰
- æä¾›å·¥å‚ç±» `ProviderFactory` ç”¨äºåˆ›å»ºæä¾›å•†å®ä¾‹

### æ¶æ„æ¨¡å¼

#### æŠ½è±¡åŸºç±»

```python
class BaseProvider(ABC):
    """AIæä¾›å•†æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def _initialize_client(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨"""
        pass

    @abstractmethod
    def chat_completion(
        self,
        messages: List[Dict],
        model: str,
        system_instruction: str = None,
        temperature: float = None,
        top_p: float = None,
        max_tokens: int = None,
        frequency_penalty: float = None,
        presence_penalty: float = None,
        stream: bool = False,
        **kwargs
    ):
        """è°ƒç”¨èŠå¤©å®Œæˆ API"""
        pass
```

#### å·¥å‚ç±»

```python
class ProviderFactory:
    """æä¾›å•†å·¥å‚ç±»"""

    _providers = {
        "cerebras": CerebrasProvider,
        "deepseek": DeepSeekProvider,
        "openai": OpenAIProvider,
        "dashscope": DashScopeProvider,
        "kimi": KimiProvider
    }

    @classmethod
    def create_provider(cls, provider_name: str) -> BaseProvider:
        """åˆ›å»ºæä¾›å•†å®ä¾‹"""
        pass

    @classmethod
    def get_available_providers(cls) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        pass

    @classmethod
    def register_provider(cls, provider_name: str, provider_class):
        """æ³¨å†Œæ–°çš„æä¾›å•†ï¼ˆæ‰©å±•ç‚¹ï¼‰"""
        pass
```

### å…·ä½“å®ç°

#### CerebrasProvider

- **SDK**: `cerebras.cloud.sdk.Cerebras`
- **ç‰¹ç‚¹**: æœ€å¿«æ¨ç†é€Ÿåº¦ï¼Œä½æˆæœ¬
- **æ¨¡å‹**: Llamaã€Qwen ç³»åˆ—

#### DeepSeekProvider

- **SDK**: `openai.OpenAI` (å…¼å®¹æ¥å£)
- **ç‰¹ç‚¹**: å¼ºå¤§çš„ä¸­æ–‡èƒ½åŠ›å’Œæ¨ç†
- **æ¨¡å‹**: deepseek-chatã€deepseek-coderã€deepseek-reasoner

#### OpenAIProvider

- **SDK**: `openai.OpenAI`
- **ç‰¹ç‚¹**: ä¸šç•Œé¢†å…ˆçš„ GPT ç³»åˆ—
- **æ¨¡å‹**: gpt-4oã€gpt-4o-miniã€gpt-4-turboã€gpt-3.5-turbo

#### DashScopeProvider

- **SDK**: `openai.OpenAI` (å…¼å®¹æ¥å£)
- **ç‰¹ç‚¹**: é˜¿é‡Œäº‘é€šä¹‰åƒé—®ç³»åˆ—
- **æ¨¡å‹**: qwen-maxã€qwen-plusã€qwen-turbo ç­‰

#### KimiProvider

- **SDK**: `openai.OpenAI` (å…¼å®¹æ¥å£)
- **ç‰¹ç‚¹**: æœˆä¹‹æš—é¢ Kimi ç³»åˆ—ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡
- **æ¨¡å‹**:
    - V1 ç³»åˆ—: moonshot-v1-8kã€moonshot-v1-32kã€moonshot-v1-128k
    - K2 ç³»åˆ—:
        - kimi-k2-0905-preview (256K ä¸Šä¸‹æ–‡)
        - kimi-k2-turbo-preview (é«˜é€Ÿç‰ˆæœ¬ï¼Œ60-100 Tokens/s)
        - kimi-k2-thinking (é•¿æ€è€ƒæ¨¡å‹ï¼Œ256K ä¸Šä¸‹æ–‡)
        - kimi-k2-thinking-turbo (é•¿æ€è€ƒé«˜é€Ÿç‰ˆæœ¬)

### ç»Ÿä¸€æ¥å£

æ‰€æœ‰æä¾›å•†å®ç°ç›¸åŒçš„ `chat_completion` æ¥å£ï¼š

```python
# éæµå¼ä¼ è¾“
response: str = provider.chat_completion(
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    model="llama-3.3-70b",
    temperature=0.7,
    stream=False
)

# æµå¼ä¼ è¾“
stream_generator = provider.chat_completion(
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    model="llama-3.3-70b",
    stream=True
)

for chunk in stream_generator:
    print(chunk, end="")
```

### æ·»åŠ æ–°æä¾›å•†

1. åˆ›å»ºæ–°ç±»ç»§æ‰¿ `BaseProvider`
2. å®ç° 3 ä¸ªæŠ½è±¡æ–¹æ³•
3. æ³¨å†Œåˆ° `ProviderFactory._providers`
4. æ›´æ–° `config.py` é…ç½®

ç¤ºä¾‹ï¼š

```python
class NewProvider(BaseProvider):
    def __init__(self):
        super().__init__("newprovider")

    def _initialize_client(self):
        config = get_provider_config(self.provider_name)
        api_key = config.get("api_key")
        if api_key:
            self.client = NewSDK(api_key=api_key)

    def is_available(self) -> bool:
        return self.client is not None

    def chat_completion(self, messages, model, **kwargs):
        # è°ƒç”¨ API
        response = self.client.chat(messages=messages, model=model)
        return response.content
```

---

## api_service.py

### æ¨¡å—èŒè´£

- ç®¡ç†å¤šä¸ªæä¾›å•†å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
- æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”æä¾›å•†
- ç»Ÿä¸€ API è°ƒç”¨æ¥å£
- æä¾›å•†çŠ¶æ€ç›‘æ§

### æ ¸å¿ƒç±»

#### MultiProviderAPIService

```python
class MultiProviderAPIService:
    """å¤šæä¾›å•† API æœåŠ¡ç±»ï¼ˆå…¨å±€å•ä¾‹ï¼‰"""

    def __init__(self):
        self.providers = {}  # Dict[str, BaseProvider]
        self._initialize_providers()

    def _initialize_providers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨æä¾›å•†"""
        # æ‰“å°åˆå§‹åŒ–çŠ¶æ€ï¼š[SUCCESS] æˆ– [FAILED]

    def chat_completion(
        self,
        messages,
        model,
        system_instruction=None,
        temperature=None,
        top_p=None,
        max_tokens=None,
        frequency_penalty=None,
        presence_penalty=None,
        stream=False,
        **kwargs
    ):
        """
        è°ƒç”¨èŠå¤©å®Œæˆ API

        è‡ªåŠ¨æ ¹æ® model è·¯ç”±åˆ°å¯¹åº”æä¾›å•†
        æ”¯æŒæµå¼å’Œéæµå¼ä¼ è¾“
        """

    def is_available(self, provider_name=None) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""

    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""

    def get_provider_status(self) -> str:
        """è·å–æ‰€æœ‰æä¾›å•†çš„çŠ¶æ€ä¿¡æ¯"""
```

### å…¨å±€å•ä¾‹

**é‡è¦**ï¼š`api_service` æ˜¯å…¨å±€å•ä¾‹å®ä¾‹ï¼Œå®šä¹‰åœ¨æ–‡ä»¶åº•éƒ¨ï¼š

```python
# src/api_service.py åº•éƒ¨
api_service = MultiProviderAPIService()
```

**ä½¿ç”¨æ–¹å¼**ï¼š

```python
# æ­£ç¡®
from src.api_service import api_service

response = api_service.chat_completion(
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    model="llama-3.3-70b"
)

# é”™è¯¯ï¼šä¸è¦åˆ›å»ºæ–°å®ä¾‹
# service = MultiProviderAPIService()  # âŒ
```

### è·¯ç”±é€»è¾‘

```python
# 1. ä»æ¨¡å‹åç§°è·å–æä¾›å•†
provider_name = get_model_provider(model)  # "cerebras"

# 2. æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
if provider_name not in self.providers:
    return "é”™è¯¯: æä¾›å•†æœªé…ç½®"

# 3. è·å–æä¾›å•†å®ä¾‹
provider = self.providers[provider_name]

# 4. è°ƒç”¨æä¾›å•† API
result = provider.chat_completion(messages, model, ...)
```

### é”™è¯¯å¤„ç†

- **æ¨¡å‹ä¸å­˜åœ¨**: è¿”å›é”™è¯¯æ¶ˆæ¯ï¼ˆä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
- **æä¾›å•†ä¸å¯ç”¨**: è¿”å›é”™è¯¯æ¶ˆæ¯ï¼ˆæç¤ºæ£€æŸ¥ API å¯†é’¥ï¼‰
- **API è°ƒç”¨å¤±è´¥**: æ•è·å¼‚å¸¸ï¼Œè¿”å›å‹å¥½é”™è¯¯æ¶ˆæ¯
- **æµå¼ä¼ è¾“**: é”™è¯¯é€šè¿‡ç”Ÿæˆå™¨ yield

### çŠ¶æ€ç›‘æ§

```python
status = api_service.get_provider_status()
# è¾“å‡º: "cerebras: [OK] å¯ç”¨ | deepseek: [OK] å¯ç”¨ | openai: [FAIL] ä¸å¯ç”¨"
```

---

## chat_manager.py

### æ¨¡å—èŒè´£

- ç»´æŠ¤å¯¹è¯å†å²è®°å½•ï¼ˆå†…å­˜å­˜å‚¨ï¼‰
- æä¾›æ¶ˆæ¯æ ¼å¼è½¬æ¢å·¥å…·ï¼ˆé¢„ç•™åŠŸèƒ½ï¼‰
- è¾…åŠ©å¯¹è¯ç®¡ç†åŠŸèƒ½

### æ ¸å¿ƒç±»

#### ChatManager

```python
class ChatManager:
    """èŠå¤©ç®¡ç†å™¨"""

    def __init__(self):
        self.history = []  # List[Dict]

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.history.append({"role": role, "content": content})

    def get_messages_for_api(self) -> List[Dict[str, str]]:
        """è·å–ç”¨äº API è°ƒç”¨çš„æ¶ˆæ¯æ ¼å¼"""
        return self.history.copy()

    def get_gradio_messages(self) -> List[Dict[str, Any]]:
        """è·å– Gradio messages æ ¼å¼çš„æ¶ˆæ¯"""
        # å½“å‰ä¸ API æ ¼å¼ç›¸åŒ

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.history.clear()

    def get_history_length(self) -> int:
        """è·å–å†å²æ¶ˆæ¯æ•°é‡"""
        return len(self.history)
```

#### MessageProcessor

```python
class MessageProcessor:
    """æ¶ˆæ¯å¤„ç†å™¨ï¼ˆé¢„ç•™æ ¼å¼è½¬æ¢ï¼‰"""

    @staticmethod
    def convert_to_api_messages(gradio_messages: List[Dict]) -> List[Dict]:
        """Gradio messages â†’ API messages"""
        # å½“å‰é€ä¼ ï¼Œé¢„ç•™æœªæ¥æ ¼å¼è½¬æ¢

    @staticmethod
    def convert_from_api_messages(api_messages: List[Dict]) -> List[Dict]:
        """API messages â†’ Gradio messages"""
        # å½“å‰é€ä¼ 

    @staticmethod
    def extract_user_messages(gradio_messages: List[Dict]) -> List[str]:
        """æå–ç”¨æˆ·æ¶ˆæ¯å†…å®¹"""

    @staticmethod
    def extract_assistant_messages(gradio_messages: List[Dict]) -> List[str]:
        """æå–åŠ©æ‰‹æ¶ˆæ¯å†…å®¹"""
```

### æ¶ˆæ¯æ ¼å¼

å½“å‰ç»Ÿä¸€ä½¿ç”¨ OpenAI æ ‡å‡†æ ¼å¼ï¼š

```python
{
    "role": "user" | "assistant" | "system",
    "content": "æ¶ˆæ¯å†…å®¹"
}
```

`MessageProcessor` ä¸ºæœªæ¥å¯èƒ½çš„æ ¼å¼å·®å¼‚é¢„ç•™ï¼Œå½“å‰æ‰€æœ‰æä¾›å•†å’Œ Gradio éƒ½ä½¿ç”¨ç›¸åŒæ ¼å¼ã€‚

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.chat_manager import ChatManager, MessageProcessor

# åˆ›å»ºç®¡ç†å™¨
manager = ChatManager()

# æ·»åŠ å¯¹è¯
manager.add_message("user", "ä½ å¥½")
manager.add_message("assistant", "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ")

# è·å– API æ ¼å¼æ¶ˆæ¯
api_messages = manager.get_messages_for_api()

# è·å– Gradio æ ¼å¼æ¶ˆæ¯
gradio_messages = manager.get_gradio_messages()

# æŸ¥è¯¢çŠ¶æ€
count = manager.get_history_length()  # 2

# æ¸…ç©ºå†å²
manager.clear_history()
```

---

## deep_think.py

### æ¨¡å—èŒè´£

- å®ç°å¤šé˜¶æ®µæ¨ç†ç³»ç»Ÿï¼ˆæ·±åº¦æ€è€ƒæ¨¡å¼ï¼‰
- ç®¡ç† 4 ä¸ªæ¨ç†é˜¶æ®µï¼šPlan â†’ Solve â†’ Synthesize â†’ Review
- æä¾›ç»“æ„åŒ–æ•°æ®æ¨¡å‹ï¼ˆdataclassï¼‰
- å®ç° Prompt æ¨¡æ¿ç®¡ç†

### æ ¸å¿ƒç±»

#### DeepThinkOrchestrator

```python
class DeepThinkOrchestrator:
    """æ·±åº¦æ€è€ƒç¼–æ’å™¨ - ç®¡ç†å¤šé˜¶æ®µæ¨ç†æµç¨‹"""

    def __init__(
        self,
        api_service,           # MultiProviderAPIService å®ä¾‹
        model: str,            # ä½¿ç”¨çš„æ¨¡å‹
        max_subtasks: int = 6, # æœ€å¤§å­ä»»åŠ¡æ•°
        enable_review: bool = True,  # æ˜¯å¦å¯ç”¨å®¡æŸ¥
        verbose: bool = True,  # æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        system_instruction: str = None,
        temperature: float = None,
        top_p: float = None,
        max_tokens: int = None
    ):
        pass

    def run(self, question: str) -> DeepThinkResult:
        """æ‰§è¡Œå®Œæ•´çš„æ·±åº¦æ€è€ƒæµç¨‹"""
        # 1. Plan é˜¶æ®µ
        plan = self._plan(question)

        # 2. Solve é˜¶æ®µï¼ˆé€ä¸ªå­ä»»åŠ¡ï¼‰
        subtask_results = []
        for subtask in plan.subtasks:
            result = self._solve_subtask(subtask, question, subtask_results)
            subtask_results.append(result)

        # 3. Synthesize é˜¶æ®µ
        final_answer = self._synthesize(question, plan, subtask_results)

        # 4. Review é˜¶æ®µï¼ˆå¯é€‰ï¼‰
        review_result = None
        if self.enable_review:
            review_result = self._review(question, final_answer)

        return DeepThinkResult(...)
```

### æ•°æ®æ¨¡å‹

#### Planï¼ˆè§„åˆ’ç»“æœï¼‰

```python
@dataclass
class Plan:
    clarified_question: str      # æ¾„æ¸…åçš„é—®é¢˜
    subtasks: List[Subtask]      # å­ä»»åŠ¡åˆ—è¡¨
    plan_text: str               # è§„åˆ’è¯´æ˜
    reasoning_approach: str = "" # æ¨ç†ç­–ç•¥
```

#### SubtaskResultï¼ˆå­ä»»åŠ¡ç»“æœï¼‰

```python
@dataclass
class SubtaskResult:
    subtask_id: int
    description: str
    analysis: str                 # åˆ†æè¿‡ç¨‹
    intermediate_conclusion: str  # ä¸­é—´ç»“è®º
    confidence: float             # ç½®ä¿¡åº¦ (0.0-1.0)
    limitations: List[str]        # å±€é™æ€§
    needs_external_info: bool = False       # æ˜¯å¦éœ€è¦å¤–éƒ¨ä¿¡æ¯
    suggested_tools: List[str] = []         # å»ºè®®çš„å·¥å…·
```

#### DeepThinkResultï¼ˆå®Œæ•´ç»“æœï¼‰

```python
@dataclass
class DeepThinkResult:
    original_question: str
    final_answer: str
    plan: Plan
    subtask_results: List[SubtaskResult]
    review: Optional[ReviewResult] = None
    total_llm_calls: int = 0
    thinking_process_summary: str = ""
```

#### ReviewResultï¼ˆå®¡æŸ¥ç»“æœï¼‰

```python
@dataclass
class ReviewResult:
    issues_found: List[str]
    improvement_suggestions: List[str]
    overall_quality_score: float  # 0.0-1.0
    review_notes: str
```

### Prompt æ¨¡æ¿

#### PromptTemplates

```python
class PromptTemplates:
    """Prompt æ¨¡æ¿é›†åˆ"""

    PLAN_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±åº¦åˆ†æå’Œè§„åˆ’ã€‚

**ç”¨æˆ·é—®é¢˜:**
{question}

**ä»»åŠ¡è¦æ±‚:**
1. ç†è§£å¹¶æ¾„æ¸…é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
2. å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸º3-6ä¸ªå¯ç®¡ç†çš„å­ä»»åŠ¡
3. ä¸ºæ¯ä¸ªå­ä»»åŠ¡è®¾å®šä¼˜å…ˆçº§(high/medium/low)
4. è§„åˆ’åˆç†çš„æ¨ç†è·¯å¾„

**è¾“å‡ºè¦æ±‚:**
è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹ç»“æ„:
...
"""

    SUBTASK_PROMPT = """..."""
    SYNTHESIZE_PROMPT = """..."""
    REVIEW_PROMPT = """..."""
```

### å·¥ä½œæµç¨‹

```
ç”¨æˆ·é—®é¢˜
    â†“
ã€Stage 1: Planã€‘
    â†“
    é—®é¢˜æ¾„æ¸… + å­ä»»åŠ¡æ‹†è§£
    â†“
ã€Stage 2: Solveã€‘
    â†“
    â”œâ”€ å­ä»»åŠ¡ 1 åˆ†æ
    â”œâ”€ å­ä»»åŠ¡ 2 åˆ†æï¼ˆåŸºäºå‰åºç»“æœï¼‰
    â”œâ”€ å­ä»»åŠ¡ 3 åˆ†æ
    â””â”€ ...
    â†“
ã€Stage 3: Synthesizeã€‘
    â†“
    æ•´åˆæ‰€æœ‰å­ä»»åŠ¡ç»“è®º â†’ æœ€ç»ˆç­”æ¡ˆ
    â†“
ã€Stage 4: Reviewã€‘ï¼ˆå¯é€‰ï¼‰
    â†“
    è´¨é‡å®¡æŸ¥ + æ”¹è¿›å»ºè®®
    â†“
DeepThinkResult
```

### JSON è§£æå®¹é”™

```python
def _parse_json_response(self, response: str) -> Dict:
    """è§£æ JSON å“åº”ï¼Œæ”¯æŒå®¹é”™å¤„ç†"""

    # 1. å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(response)
    except: pass

    # 2. å°è¯•æå– ```json``` ä»£ç å—
    if "```json" in response:
        json_block = response.split("```json")[1].split("```")[0].strip()
        try:
            return json.loads(json_block)
        except: pass

    # 3. å°è¯•æŸ¥æ‰¾èŠ±æ‹¬å·å†…çš„å†…å®¹
    start = response.find("{")
    end = response.rfind("}") + 1
    if start != -1 and end > start:
        try:
            return json.loads(response[start:end])
        except: pass

    # 4. å¦‚æœéƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise ValueError("æ— æ³•è§£æJSONå“åº”")
```

### æ ¼å¼åŒ–è¾“å‡º

```python
def format_deep_think_result(
    result: DeepThinkResult,
    include_process: bool = True
) -> str:
    """æ ¼å¼åŒ–æ·±åº¦æ€è€ƒç»“æœä¸ºç”¨æˆ·å‹å¥½çš„ Markdown è¾“å‡º"""

    output = []

    # ä¸»è¦ç­”æ¡ˆ
    output.append("# ğŸ’¡ æ·±åº¦æ€è€ƒç»“æœ\n")
    output.append(result.final_answer)

    # æ€è€ƒè¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰
    if include_process:
        output.append(f"\n\n{result.thinking_process_summary}")

    # å®¡æŸ¥ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    if result.review:
        output.append("\n\n## ğŸ” è´¨é‡å®¡æŸ¥")
        output.append(f"**æ•´ä½“è¯„åˆ†:** {result.review.overall_quality_score:.0%}")
        # ...

    # å…ƒä¿¡æ¯
    output.append(f"\n\n---\n*æ·±åº¦æ€è€ƒæ¨¡å¼ | LLMè°ƒç”¨æ¬¡æ•°: {result.total_llm_calls}*")

    return "\n".join(output)
```

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.api_service import api_service
from src.deep_think import DeepThinkOrchestrator, format_deep_think_result

# åˆ›å»ºç¼–æ’å™¨
orchestrator = DeepThinkOrchestrator(
    api_service=api_service,
    model="qwen-3-235b-a22b-thinking-2507",
    max_subtasks=6,
    enable_review=True,
    verbose=True
)

# æ‰§è¡Œæ·±åº¦æ€è€ƒ
question = "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿè¯·ä»ç†è®ºå­¦ä¹ ã€å®è·µé¡¹ç›®ã€æŒç»­æ”¹è¿›ä¸‰ä¸ªè§’åº¦åˆ†æã€‚"
result = orchestrator.run(question)

# æ ¼å¼åŒ–è¾“å‡º
formatted = format_deep_think_result(result, include_process=True)
print(formatted)

# è®¿é—®è¯¦ç»†æ•°æ®
print(f"å­ä»»åŠ¡æ•°é‡: {len(result.subtask_results)}")
print(f"LLM è°ƒç”¨æ¬¡æ•°: {result.total_llm_calls}")
if result.review:
    print(f"è´¨é‡è¯„åˆ†: {result.review.overall_quality_score:.2%}")
```

### æ€§èƒ½æŒ‡æ ‡

- **LLM è°ƒç”¨æ¬¡æ•°**: 5-9 æ¬¡ï¼ˆ1 è§„åˆ’ + N åˆ†æ + 1 æ•´åˆ + 1 å®¡æŸ¥ï¼‰
- **Token æ¶ˆè€—**: çº¦ 12,000 tokens/ä¼šè¯
- **å“åº”æ—¶é—´**: 30-180 ç§’
- **æˆæœ¬**: < $0.01/æ¬¡ï¼ˆCerebrasï¼‰

### æ‰©å±•ç‚¹

ç³»ç»Ÿé¢„ç•™äº†å·¥å…·è°ƒç”¨æ¥å£ï¼š

```python
# SubtaskResult ä¸­çš„æ‰©å±•å­—æ®µ
needs_external_info: bool = False       # æ ‡è®°æ˜¯å¦éœ€è¦å¤–éƒ¨ä¿¡æ¯
suggested_tools: List[str] = []         # å»ºè®®çš„å·¥å…·ï¼ˆå¦‚ "search", "rag"ï¼‰
```

æœªæ¥å¯åœ¨ `_solve_subtask` ä¸­é›†æˆï¼š

- æœç´¢å¼•æ“
- RAG ç³»ç»Ÿ
- ä»£ç æ‰§è¡Œ
- API è°ƒç”¨

---

## æ¨¡å—é—´ä¾èµ–

### å¯¼å…¥å…³ç³»

```python
# config.py - æ— ä¾èµ–
import os
from dotenv import load_dotenv

# providers.py - ä¾èµ– config
from .config import get_provider_config
from cerebras.cloud.sdk import Cerebras
from openai import OpenAI

# api_service.py - ä¾èµ– config, providers
from .config import get_model_provider
from .providers import ProviderFactory

# chat_manager.py - æ— ä¾èµ–
from typing import List, Dict, Any

# deep_think.py - ä¾èµ– api_serviceï¼ˆé€šè¿‡å‚æ•°æ³¨å…¥ï¼‰
import json
import logging
from dataclasses import dataclass
```

### åˆå§‹åŒ–é¡ºåº

```
1. config.py åŠ è½½ç¯å¢ƒå˜é‡
2. providers.py å®šä¹‰æä¾›å•†ç±»
3. api_service.py åˆ›å»ºå…¨å±€å•ä¾‹ï¼ˆåˆå§‹åŒ–æä¾›å•†ï¼‰
4. chat_manager.py ç‹¬ç«‹åˆå§‹åŒ–
5. deep_think.py æ¥æ”¶ api_service å®ä¾‹
```

---

## å¼€å‘æŒ‡å—

### ä¿®æ”¹é…ç½®

**æ·»åŠ æ–°æ¨¡å‹**ï¼š

```python
# src/config.py
PROVIDER_MODELS["cerebras"].append("new-model-name")
```

**æ·»åŠ æ–°æä¾›å•†**ï¼š

1. åœ¨ `providers.py` åˆ›å»ºç±»
2. æ³¨å†Œåˆ° `ProviderFactory`
3. åœ¨ `config.py` æ·»åŠ é…ç½®

### ä¿®æ”¹æ·±åº¦æ€è€ƒ

**è°ƒæ•´ Prompt**ï¼š

```python
# src/deep_think.py
class PromptTemplates:
    PLAN_PROMPT = """ä¿®æ”¹åçš„ Prompt..."""
```

**ä¿®æ”¹é˜¶æ®µé€»è¾‘**ï¼š

```python
# src/deep_think.py
class DeepThinkOrchestrator:
    def _plan(self, question: str) -> Plan:
        # ä¿®æ”¹è§„åˆ’é€»è¾‘
        pass
```

### æµ‹è¯•å»ºè®®

- **å•å…ƒæµ‹è¯•**: æµ‹è¯• `config.py` å·¥å…·å‡½æ•°
- **é›†æˆæµ‹è¯•**: æµ‹è¯• `api_service.py` æä¾›å•†è·¯ç”±
- **ç«¯åˆ°ç«¯æµ‹è¯•**: æµ‹è¯• `deep_think.py` å®Œæ•´æµç¨‹

---

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•åˆ‡æ¢é»˜è®¤æ¨¡å‹ï¼Ÿ

ä¿®æ”¹ `src/config.py`:

```python
DEFAULT_MODEL = "your-model-name"
DEFAULT_PROVIDER = "your-provider-name"
```

### Q: å¦‚ä½•ç¦ç”¨æŸä¸ªæä¾›å•†ï¼Ÿ

ä¿®æ”¹ `src/config.py`:

```python
PROVIDER_CONFIG["provider_name"]["enabled"] = False
```

### Q: ä¸ºä»€ä¹ˆä¸èƒ½åˆ›å»ºå¤šä¸ª api_service å®ä¾‹ï¼Ÿ

å› ä¸ºé‡‡ç”¨å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åˆå§‹åŒ–æä¾›å•†ã€‚æ­£ç¡®ç”¨æ³•ï¼š

```python
from src.api_service import api_service  # ä½¿ç”¨å…¨å±€å®ä¾‹
```

### Q: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰å‚æ•°åˆ° API è°ƒç”¨ï¼Ÿ

é€šè¿‡ `**kwargs` ä¼ é€’ï¼š

```python
response = api_service.chat_completion(
    messages=messages,
    model=model,
    custom_param="value",  # è‡ªå®šä¹‰å‚æ•°
    **kwargs
)
```

### Q: æ·±åº¦æ€è€ƒå¯ä»¥ä½¿ç”¨æµå¼ä¼ è¾“å—ï¼Ÿ

æš‚ä¸æ”¯æŒï¼Œå› ä¸ºå¤šé˜¶æ®µæ¨ç†éœ€è¦å®Œæ•´å“åº”æ‰èƒ½è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚æœªæ¥å¯å®ç°é˜¶æ®µæ€§æµå¼è¾“å‡ºã€‚

---

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶

- `__init__.py` - åŒ…æ ‡è¯†
- `config.py` - é…ç½®ç®¡ç†
- `providers.py` - æä¾›å•†å®ç°
- `api_service.py` - API ç¼–æ’
- `chat_manager.py` - å¯¹è¯ç®¡ç†
- `deep_think.py` - æ·±åº¦æ€è€ƒ

### ä¾èµ–çš„å¤–éƒ¨æ–‡ä»¶

- `../.env` - API å¯†é’¥é…ç½®
- `../main.py` - UI å…¥å£ï¼ˆä½¿ç”¨ src.*ï¼‰

### ç›¸å…³æ–‡æ¡£

- `../doc/deep_thinking_feature.md` - æ·±åº¦æ€è€ƒå®Œæ•´æ–‡æ¡£
- `../CLAUDE.md` - æ ¹çº§ AI æŒ‡å¼•

---

## å˜æ›´è®°å½• (Changelog)

### 2025-12-01 15:07:03

- åˆå§‹åŒ– src/ æ¨¡å—æ–‡æ¡£
- è¯¦ç»†è¯´æ˜å„æ¨¡å—èŒè´£å’Œæ¥å£
- æ·»åŠ ä½¿ç”¨ç¤ºä¾‹å’Œå¼€å‘æŒ‡å—

---

[è¿”å›æ ¹ç›®å½•](../CLAUDE.md)
