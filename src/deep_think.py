"""
æ·±åº¦æ€è€ƒæ¨¡å— - å®ç°å¤šé˜¶æ®µæ¨ç†å’Œæ·±åº¦ç ”ç©¶èƒ½åŠ›
"""

import hashlib
import json
import logging
import threading
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ThinkingStage(Enum):
    """æ·±åº¦æ€è€ƒçš„å„ä¸ªé˜¶æ®µ"""

    PLAN = "plan"
    SOLVE = "solve"
    SYNTHESIZE = "synthesize"
    REVIEW = "review"


@dataclass
class Subtask:
    """å­ä»»åŠ¡æ•°æ®ç»“æ„"""

    id: int
    description: str
    priority: str = "medium"  # high, medium, low
    dependencies: List[int] = field(default_factory=list)


@dataclass
class Plan:
    """ä»»åŠ¡è§„åˆ’ç»“æœ"""

    clarified_question: str
    subtasks: List[Subtask]
    plan_text: str
    reasoning_approach: str = ""


@dataclass
class SubtaskResult:
    """å­ä»»åŠ¡æ‰§è¡Œç»“æœ"""

    subtask_id: int
    description: str
    analysis: str
    intermediate_conclusion: str
    confidence: float  # 0.0 - 1.0
    limitations: List[str] = field(default_factory=list)
    needs_external_info: bool = False  # æ˜¯å¦éœ€è¦å¤–éƒ¨ä¿¡æ¯(é¢„ç•™å·¥å…·è°ƒç”¨)
    suggested_tools: List[str] = field(default_factory=list)  # å»ºè®®ä½¿ç”¨çš„å·¥å…·


@dataclass
class ReviewResult:
    """å®¡æŸ¥ç»“æœ"""

    issues_found: List[str]
    improvement_suggestions: List[str]
    overall_quality_score: float  # 0.0 - 1.0
    review_notes: str


@dataclass
class DeepThinkResult:
    """æ·±åº¦æ€è€ƒå®Œæ•´ç»“æœ"""

    original_question: str
    final_answer: str
    plan: Plan
    subtask_results: List[SubtaskResult]
    review: Optional[ReviewResult] = None
    total_llm_calls: int = 0
    thinking_process_summary: str = ""


class PromptTemplates:
    """Promptæ¨¡æ¿é›†åˆ"""

    PLAN_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±åº¦åˆ†æå’Œè§„åˆ’ã€‚

**ç”¨æˆ·é—®é¢˜:**
{question}

**ä»»åŠ¡è¦æ±‚:**
1. ç†è§£å¹¶æ¾„æ¸…é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
2. å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸º3-6ä¸ªå¯ç®¡ç†çš„å­ä»»åŠ¡
3. ä¸ºæ¯ä¸ªå­ä»»åŠ¡è®¾å®šä¼˜å…ˆçº§(high/medium/low)
4. è§„åˆ’åˆç†çš„æ¨ç†è·¯å¾„

**è¾“å‡ºè¦æ±‚:**
è¯·ä»¥JSONæ ¼å¼è¾“å‡º,ä¸¥æ ¼éµå¾ªä»¥ä¸‹ç»“æ„:
{{
    "clarified_question": "æ¾„æ¸…åçš„é—®é¢˜æè¿°",
    "reasoning_approach": "æ€»ä½“æ¨ç†ç­–ç•¥è¯´æ˜",
    "subtasks": [
        {{
            "id": 1,
            "description": "å­ä»»åŠ¡æè¿°",
            "priority": "high|medium|low",
            "dependencies": []
        }}
    ],
    "plan_text": "æ•´ä½“è§„åˆ’çš„è‡ªç„¶è¯­è¨€æè¿°"
}}

åªè¿”å›JSON,ä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

    SUBTASK_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹å­ä»»åŠ¡è¿›è¡Œæ·±å…¥åˆ†æã€‚

**åŸå§‹é—®é¢˜:** {original_question}

**å½“å‰å­ä»»åŠ¡:**
{subtask_description}

**å·²å®Œæˆçš„ç›¸å…³å­ä»»åŠ¡ç»“è®º:**
{previous_conclusions}

**åˆ†æè¦æ±‚:**
1. æ·±å…¥åˆ†æè¿™ä¸ªå­ä»»åŠ¡
2. åŸºäºå·²çŸ¥ä¿¡æ¯ç»™å‡ºä¸­é—´ç»“è®º
3. è¯„ä¼°ç»“è®ºçš„å¯ä¿¡åº¦
4. è¯†åˆ«åˆ†æçš„å±€é™æ€§
5. åˆ¤æ–­æ˜¯å¦éœ€è¦å¤–éƒ¨ä¿¡æ¯(å¦‚æœç´¢ã€æ•°æ®æŸ¥è¯¢ç­‰)

**è¾“å‡ºè¦æ±‚:**
è¯·ä»¥JSONæ ¼å¼è¾“å‡º:
{{
    "analysis": "è¯¦ç»†çš„åˆ†æè¿‡ç¨‹",
    "intermediate_conclusion": "è¯¥å­ä»»åŠ¡çš„ç»“è®º",
    "confidence": 0.85,
    "limitations": ["å±€é™æ€§1", "å±€é™æ€§2"],
    "needs_external_info": false,
    "suggested_tools": []
}}

åªè¿”å›JSON,ä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

    SYNTHESIZE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ•´åˆä¸“å®¶ã€‚è¯·åŸºäºæ‰€æœ‰å­ä»»åŠ¡çš„ç»“è®º,ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

**åŸå§‹é—®é¢˜:**
{original_question}

**æ¾„æ¸…åçš„é—®é¢˜:**
{clarified_question}

**æ¨ç†ç­–ç•¥:**
{reasoning_approach}

**æ‰€æœ‰å­ä»»åŠ¡çš„ç»“è®º:**
{all_conclusions}

**æ•´åˆè¦æ±‚:**
1. ç»¼åˆæ‰€æœ‰å­ä»»åŠ¡çš„ç»“è®º
2. å½¢æˆè¿è´¯ã€å®Œæ•´çš„æœ€ç»ˆç­”æ¡ˆ
3. ä¿æŒé€»è¾‘ä¸¥å¯†æ€§
4. æ ‡æ³¨ä¸ç¡®å®šçš„éƒ¨åˆ†
5. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„(å¦‚åˆ†æ®µã€åˆ—è¡¨ç­‰)

**è¾“å‡ºè¦æ±‚:**
è¯·ä»¥JSONæ ¼å¼è¾“å‡º:
{{
    "final_answer": "ç»“æ„åŒ–çš„æœ€ç»ˆç­”æ¡ˆ,ä½¿ç”¨Markdownæ ¼å¼",
    "synthesis_notes": "æ•´åˆè¿‡ç¨‹çš„è¯´æ˜",
    "confidence_areas": {{
        "high_confidence": ["ç¡®å®šæ€§é«˜çš„ç»“è®º"],
        "medium_confidence": ["ä¸­ç­‰ç¡®å®šæ€§çš„ç»“è®º"],
        "low_confidence": ["éœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„ç»“è®º"]
    }}
}}

åªè¿”å›JSON,ä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

    REVIEW_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è´¨é‡å®¡æŸ¥ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç­”æ¡ˆè¿›è¡Œæ‰¹åˆ¤æ€§å®¡æŸ¥ã€‚

**åŸå§‹é—®é¢˜:**
{original_question}

**å¾…å®¡æŸ¥çš„ç­”æ¡ˆ:**
{final_answer}

**å®¡æŸ¥è¦æ±‚:**
1. æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§
2. è¯†åˆ«æ½œåœ¨çš„é”™è¯¯æˆ–é—æ¼
3. è¯„ä¼°ç­”æ¡ˆçš„å®Œæ•´æ€§
4. æå‡ºæ”¹è¿›å»ºè®®
5. ç»™å‡ºæ•´ä½“è´¨é‡è¯„åˆ†(0.0-1.0)

**è¾“å‡ºè¦æ±‚:**
è¯·ä»¥JSONæ ¼å¼è¾“å‡º:
{{
    "issues_found": ["é—®é¢˜1", "é—®é¢˜2"],
    "improvement_suggestions": ["æ”¹è¿›å»ºè®®1", "å»ºè®®2"],
    "overall_quality_score": 0.85,
    "review_notes": "æ€»ä½“å®¡æŸ¥æ„è§"
}}

åªè¿”å›JSON,ä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""


class DeepThinkOrchestrator:
    """æ·±åº¦æ€è€ƒç¼–æ’å™¨ - ç®¡ç†å¤šé˜¶æ®µæ¨ç†æµç¨‹"""

    def __init__(
            self,
            api_service,
            model: str,
            max_subtasks: int = 6,
            enable_review: bool = True,
            verbose: bool = True,
            system_instruction: Optional[str] = None,
            temperature: Optional[float] = None,
            top_p: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–æ·±åº¦æ€è€ƒç¼–æ’å™¨

        Args:
            api_service: MultiProviderAPIServiceå®ä¾‹
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            max_subtasks: æœ€å¤§å­ä»»åŠ¡æ•°é‡
            enable_review: æ˜¯å¦å¯ç”¨æœ€ç»ˆå®¡æŸ¥
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            system_instruction: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            top_p: æ ¸é‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_service = api_service
        self.model = model
        self.max_subtasks = max_subtasks
        self.enable_review = enable_review
        self.verbose = verbose
        self.llm_call_count = 0

        # æ¨¡å‹å‚æ•°
        self.system_instruction = system_instruction
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens

        # æ·»åŠ ç¼“å­˜åŠŸèƒ½
        self.intermediate_cache = {}
        self.cache_lock = threading.Lock()

    def run(self, question: str) -> DeepThinkResult:
        """
        æ‰§è¡Œæ·±åº¦æ€è€ƒæµç¨‹

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            DeepThinkResult: å®Œæ•´çš„æ€è€ƒç»“æœ
        """
        logger.info(f"[DEEP THINK] å¼€å§‹æ·±åº¦æ€è€ƒæ¨¡å¼: {question[:50]}...")

        try:
            # é˜¶æ®µ1: è§„åˆ’
            plan = self._plan(question)
            logger.info(f"[PLAN] ç”Ÿæˆäº† {len(plan.subtasks)} ä¸ªå­ä»»åŠ¡")

            # é˜¶æ®µ2: é€ä¸ªè§£å†³å­ä»»åŠ¡
            subtask_results = []
            for subtask in plan.subtasks:
                result = self._solve_subtask(subtask, question, subtask_results)
                subtask_results.append(result)
                logger.info(f"[SOLVE] å®Œæˆå­ä»»åŠ¡ {subtask.id}: {subtask.description[:30]}...")

            # é˜¶æ®µ3: æ•´åˆç»“æœ
            final_answer = self._synthesize(question, plan, subtask_results)
            logger.info("[SYNTHESIZE] ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")

            # é˜¶æ®µ4: å¯é€‰å®¡æŸ¥
            review_result = None
            if self.enable_review:
                review_result = self._review(question, final_answer)
                logger.info(
                    f"[REVIEW] å®¡æŸ¥å®Œæˆ,è´¨é‡è¯„åˆ†: {review_result.overall_quality_score:.2f}"
                )

            # ç”Ÿæˆæ€è€ƒè¿‡ç¨‹æ‘˜è¦
            thinking_summary = self._generate_thinking_summary(plan, subtask_results)

            result = DeepThinkResult(
                original_question=question,
                final_answer=final_answer,
                plan=plan,
                subtask_results=subtask_results,
                review=review_result,
                total_llm_calls=self.llm_call_count,
                thinking_process_summary=thinking_summary,
            )

            logger.info(f"[DEEP THINK] å®Œæˆ,å…±è°ƒç”¨LLM {self.llm_call_count} æ¬¡")
            return result

        except Exception as e:
            logger.error(f"[DEEP THINK] æ‰§è¡Œå¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªé”™è¯¯ç»“æœ
            return DeepThinkResult(
                original_question=question,
                final_answer=f"æ·±åº¦æ€è€ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e!s}",
                plan=Plan(clarified_question=question, subtasks=[], plan_text=""),
                subtask_results=[],
                total_llm_calls=self.llm_call_count,
            )

    def _get_cache_key(self, method_name: str, *args, **kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_input = {
            "method": method_name,
            "args": args,
            "kwargs": {k: v for k, v in kwargs.items() if k != "self"},
        }
        cache_str = str(sorted(cache_input.items()))
        return hashlib.md5(cache_str.encode()).hexdigest()

    def _get_from_cache(self, key):
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        with self.cache_lock:
            return self.intermediate_cache.get(key)

    def _set_to_cache(self, key, value):
        """è®¾ç½®ç¼“å­˜"""
        with self.cache_lock:
            self.intermediate_cache[key] = value

    def _plan(self, question: str) -> Plan:
        """ç”Ÿæˆä»»åŠ¡è§„åˆ’"""
        # å°è¯•ä»ç¼“å­˜è·å–
        cache_key = self._get_cache_key("_plan", question)
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            if self.verbose:
                logger.info("[PLAN] ä»ç¼“å­˜è·å–è§„åˆ’")
            return cached_result

        prompt = PromptTemplates.PLAN_PROMPT.format(question=question)
        response = self._call_llm(prompt, stage=ThinkingStage.PLAN)

        try:
            plan_data = self._parse_json_response(response)

            subtasks = [
                Subtask(
                    id=st["id"],
                    description=st["description"],
                    priority=st.get("priority", "medium"),
                    dependencies=st.get("dependencies", []),
                )
                for st in plan_data.get("subtasks", [])[: self.max_subtasks]
            ]

            result = Plan(
                clarified_question=plan_data.get("clarified_question", question),
                subtasks=subtasks,
                plan_text=plan_data.get("plan_text", ""),
                reasoning_approach=plan_data.get("reasoning_approach", ""),
            )

            # å­˜å‚¨åˆ°ç¼“å­˜
            self._set_to_cache(cache_key, result)
            return result

        except Exception as e:
            logger.warning(f"[PLAN] JSONè§£æå¤±è´¥,ä½¿ç”¨é»˜è®¤è§„åˆ’: {e}")
            # å®¹é”™: åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ç®€å•è§„åˆ’
            result = Plan(
                clarified_question=question,
                subtasks=[
                    Subtask(id=1, description="æ·±å…¥ç†è§£å’Œåˆ†æé—®é¢˜", priority="high"),
                    Subtask(id=2, description="æ¢ç´¢å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ", priority="medium"),
                    Subtask(id=3, description="ç»¼åˆè¯„ä¼°å’Œæ€»ç»“", priority="medium"),
                ],
                plan_text="ç”±äºè§„åˆ’è§£æå¤±è´¥,ä½¿ç”¨é»˜è®¤ä¸‰é˜¶æ®µåˆ†ææµç¨‹",
            )

            # å­˜å‚¨åˆ°ç¼“å­˜
            self._set_to_cache(cache_key, result)
            return result

    def _solve_subtask(
            self, subtask: Subtask, original_question: str, previous_results: List[SubtaskResult]
    ) -> SubtaskResult:
        """è§£å†³å•ä¸ªå­ä»»åŠ¡"""
        # æ„å»ºä¹‹å‰çš„ç»“è®ºä¸Šä¸‹æ–‡
        previous_conclusions = (
            "\n".join(
                [f"- å­ä»»åŠ¡{r.subtask_id}: {r.intermediate_conclusion}" for r in previous_results]
            )
            if previous_results
            else "æš‚æ— "
        )

        prompt = PromptTemplates.SUBTASK_PROMPT.format(
            original_question=original_question,
            subtask_description=subtask.description,
            previous_conclusions=previous_conclusions,
        )

        response = self._call_llm(prompt, stage=ThinkingStage.SOLVE)

        try:
            result_data = self._parse_json_response(response)

            return SubtaskResult(
                subtask_id=subtask.id,
                description=subtask.description,
                analysis=result_data.get("analysis", response),
                intermediate_conclusion=result_data.get("intermediate_conclusion", ""),
                confidence=float(result_data.get("confidence", 0.7)),
                limitations=result_data.get("limitations", []),
                needs_external_info=result_data.get("needs_external_info", False),
                suggested_tools=result_data.get("suggested_tools", []),
            )

        except Exception as e:
            logger.warning(f"[SOLVE] å­ä»»åŠ¡ {subtask.id} JSONè§£æå¤±è´¥: {e}")
            # å®¹é”™: ä½¿ç”¨åŸå§‹å“åº”ä½œä¸ºåˆ†æç»“æœ
            # ç¡®ä¿ response æ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
            if not isinstance(response, str):
                response = str(response)

            conclusion = response.strip()
            if len(conclusion) > 200:
                conclusion = conclusion[:200] + "..."
            elif not conclusion:
                conclusion = "ï¼ˆå­ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œä½†æœªèƒ½æå–ç»“è®ºï¼‰"

            return SubtaskResult(
                subtask_id=subtask.id,
                description=subtask.description,
                analysis=response,
                intermediate_conclusion=conclusion,
                confidence=0.6,
                limitations=["JSONè§£æå¤±è´¥,ä½¿ç”¨åŸå§‹å“åº”"],
            )

    def _synthesize(
            self, original_question: str, plan: Plan, subtask_results: List[SubtaskResult]
    ) -> str:
        """æ•´åˆæ‰€æœ‰å­ä»»åŠ¡ç»“æœ"""
        all_conclusions = "\n\n".join(
            [
                f"**å­ä»»åŠ¡ {r.subtask_id}: {r.description}**\n"
                f"ç»“è®º: {r.intermediate_conclusion}\n"
                f"å¯ä¿¡åº¦: {r.confidence:.0%}\n"
                f"å±€é™æ€§: {', '.join(r.limitations) if r.limitations else 'æ— '}"
                for r in subtask_results
            ]
        )

        prompt = PromptTemplates.SYNTHESIZE_PROMPT.format(
            original_question=original_question,
            clarified_question=plan.clarified_question,
            reasoning_approach=plan.reasoning_approach,
            all_conclusions=all_conclusions,
        )

        response = self._call_llm(prompt, stage=ThinkingStage.SYNTHESIZE)

        try:
            synthesis_data = self._parse_json_response(response)
            final_answer = synthesis_data.get("final_answer", response)

            # æ·»åŠ æ•´åˆè¯´æ˜(å¯é€‰)
            if "synthesis_notes" in synthesis_data:
                final_answer += f"\n\n---\n**æ•´åˆè¯´æ˜:** {synthesis_data['synthesis_notes']}"

            return final_answer

        except Exception as e:
            logger.warning(f"[SYNTHESIZE] JSONè§£æå¤±è´¥: {e}")
            # å®¹é”™: ç›´æ¥ä½¿ç”¨å“åº”
            # ç¡®ä¿å“åº”ä¸ä¸ºç©º
            if response and response.strip():
                return response
            else:
                # å¦‚æœå“åº”ä¸ºç©ºï¼Œè¿”å›åŸºäºå­ä»»åŠ¡ç»“è®ºçš„å›é€€ç­”æ¡ˆ
                logger.warning("[SYNTHESIZE] å“åº”ä¸ºç©ºï¼Œä½¿ç”¨å›é€€ç­”æ¡ˆ")
                fallback_parts = ["åŸºäºä¸Šè¿°åˆ†æï¼Œç»¼åˆç»“è®ºå¦‚ä¸‹ï¼š\n"]
                for r in subtask_results:
                    fallback_parts.append(f"- {r.description}: {r.intermediate_conclusion[:100]}")
                return "\n".join(fallback_parts)

    def _review(self, original_question: str, final_answer: str) -> ReviewResult:
        """å®¡æŸ¥æœ€ç»ˆç­”æ¡ˆ"""
        prompt = PromptTemplates.REVIEW_PROMPT.format(
            original_question=original_question, final_answer=final_answer
        )

        response = self._call_llm(prompt, stage=ThinkingStage.REVIEW)

        try:
            review_data = self._parse_json_response(response)

            return ReviewResult(
                issues_found=review_data.get("issues_found", []),
                improvement_suggestions=review_data.get("improvement_suggestions", []),
                overall_quality_score=float(review_data.get("overall_quality_score", 0.75)),
                review_notes=review_data.get("review_notes", ""),
            )

        except Exception as e:
            logger.warning(f"[REVIEW] JSONè§£æå¤±è´¥: {e}")
            # å®¹é”™: è¿”å›é»˜è®¤å®¡æŸ¥ç»“æœ
            return ReviewResult(
                issues_found=[],
                improvement_suggestions=[],
                overall_quality_score=0.7,
                review_notes="å®¡æŸ¥æ•°æ®è§£æå¤±è´¥",
            )

    def _call_llm(self, prompt: str, stage: ThinkingStage) -> str:
        """è°ƒç”¨LLM"""
        self.llm_call_count += 1

        if self.verbose:
            logger.info(f"[LLM CALL #{self.llm_call_count}] Stage: {stage.value}")

        messages = [{"role": "user", "content": prompt}]

        response = self.api_service.chat_completion(
            messages=messages,
            model=self.model,
            system_instruction=self.system_instruction,
            temperature=self.temperature,
            top_p=self.top_p,
            max_tokens=self.max_tokens,
            stream=False,  # æ·±åº¦æ€è€ƒæ¨¡å¼å¿…é¡»ä½¿ç”¨éæµå¼ä¼ è¾“
        )

        # ç¡®ä¿è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ç±»å‹
        # å¦‚æœ API è¿”å›äº†ç”Ÿæˆå™¨ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†åšé˜²æŠ¤ï¼‰ï¼Œå°†å…¶å®Œå…¨æ¶ˆè´¹
        if hasattr(response, '__iter__') and not isinstance(response, (str, bytes)):
            if self.verbose:
                logger.warning("[LLM CALL] æ£€æµ‹åˆ°ç”Ÿæˆå™¨å“åº”ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²...")
            try:
                response = ''.join(str(chunk) for chunk in response)
            except Exception as e:
                error_msg = f"æ— æ³•å°†ç”Ÿæˆå™¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²: {e}"
                logger.error(f"[LLM CALL] {error_msg}")
                raise TypeError(error_msg)

        # æœ€ç»ˆç±»å‹æ£€æŸ¥
        if not isinstance(response, str):
            raise TypeError(f"API å“åº”å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ {type(response).__name__}")

        # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå“åº”çš„å‰200ä¸ªå­—ç¬¦
        if self.verbose:
            preview = response[:200].replace('\n', ' ')
            logger.info(f"[LLM RESPONSE] {preview}{'...' if len(response) > 200 else ''}")

        return response

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """è§£æJSONå“åº”,æ”¯æŒå®¹é”™å¤„ç†"""
        # é˜²æŠ¤ï¼šå¦‚æœæ”¶åˆ°ç”Ÿæˆå™¨å¯¹è±¡ï¼Œå°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if hasattr(response, '__iter__') and not isinstance(response, (str, bytes)):
            try:
                response = ''.join(response)
            except Exception as e:
                raise TypeError(f"å“åº”å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ {type(response).__name__}: {e}")

        # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(response, str):
            raise TypeError(f"å“åº”å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ {type(response).__name__}")

        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # å°è¯•æå–JSONä»£ç å—
        if "```json" in response:
            json_block = response.split("```json")[1].split("```")[0].strip()
            try:
                return json.loads(json_block)
            except json.JSONDecodeError:
                pass

        # å°è¯•æŸ¥æ‰¾èŠ±æ‹¬å·å†…çš„å†…å®¹
        start = response.find("{")
        end = response.rfind("}") + 1
        if start != -1 and end > start:
            try:
                return json.loads(response[start:end])
            except json.JSONDecodeError:
                pass

        # å¦‚æœéƒ½å¤±è´¥,æŠ›å‡ºå¼‚å¸¸
        raise ValueError(f"æ— æ³•è§£æJSONå“åº”: {response[:100]}...")

    def _generate_thinking_summary(self, plan: Plan, subtask_results: List[SubtaskResult]) -> str:
        """ç”Ÿæˆæ€è€ƒè¿‡ç¨‹æ‘˜è¦"""
        summary_parts = [
            "## ğŸ§  æ·±åº¦æ€è€ƒè¿‡ç¨‹æ‘˜è¦\n",
            f"**é—®é¢˜æ¾„æ¸…:** {plan.clarified_question}\n",
            f"**æ¨ç†ç­–ç•¥:** {plan.reasoning_approach}\n",
            "\n**å­ä»»åŠ¡æ‰§è¡Œæƒ…å†µ:**",
        ]

        for result in subtask_results:
            # æ™ºèƒ½æˆªæ–­ï¼šåªåœ¨è¶…è¿‡100å­—ç¬¦æ—¶æ‰æ·»åŠ çœç•¥å·
            conclusion = result.intermediate_conclusion
            if len(conclusion) > 100:
                conclusion_display = conclusion[:100] + "..."
            else:
                conclusion_display = conclusion

            summary_parts.append(
                f"\n{result.subtask_id}. {result.description}\n"
                f"   - å¯ä¿¡åº¦: {result.confidence:.0%}\n"
                f"   - ç»“è®º: {conclusion_display}"
            )

        return "\n".join(summary_parts)


def format_deep_think_result(result: DeepThinkResult, include_process: bool = True) -> str:
    """
    æ ¼å¼åŒ–æ·±åº¦æ€è€ƒç»“æœä¸ºç”¨æˆ·å‹å¥½çš„è¾“å‡º

    Args:
        result: DeepThinkResultå®ä¾‹
        include_process: æ˜¯å¦åŒ…å«æ€è€ƒè¿‡ç¨‹è¯¦æƒ…

    Returns:
        str: æ ¼å¼åŒ–çš„Markdownæ–‡æœ¬
    """
    output_parts = []

    # ä¸»è¦ç­”æ¡ˆ
    output_parts.append("# ğŸ’¡ æ·±åº¦æ€è€ƒç»“æœ\n")

    # ç¡®ä¿ final_answer ä¸ä¸ºç©º
    if result.final_answer and result.final_answer.strip():
        output_parts.append(result.final_answer)
    else:
        output_parts.append("âš ï¸ **æœªèƒ½ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ**\n\nå¯èƒ½åŸå› ï¼š")
        output_parts.append("- æ¨¡å‹æœªè¿”å›ç¬¦åˆé¢„æœŸçš„ JSON æ ¼å¼")
        output_parts.append("- API è°ƒç”¨è¶…æ—¶æˆ–å¤±è´¥")
        output_parts.append("\nè¯·æŸ¥çœ‹ä¸‹æ–¹çš„æ€è€ƒè¿‡ç¨‹æ‘˜è¦äº†è§£è¯¦æƒ…ã€‚")

    # æ€è€ƒè¿‡ç¨‹(å¯é€‰)
    if include_process and result.thinking_process_summary:
        output_parts.append(f"\n\n{result.thinking_process_summary}")

    # å®¡æŸ¥ç»“æœ(å¦‚æœæœ‰)
    if result.review:
        output_parts.append("\n\n## ğŸ” è´¨é‡å®¡æŸ¥")
        output_parts.append(f"**æ•´ä½“è¯„åˆ†:** {result.review.overall_quality_score:.0%}")

        if result.review.issues_found:
            output_parts.append("\n**å‘ç°çš„é—®é¢˜:**")
            for issue in result.review.issues_found:
                output_parts.append(f"- {issue}")

        if result.review.improvement_suggestions:
            output_parts.append("\n**æ”¹è¿›å»ºè®®:**")
            for suggestion in result.review.improvement_suggestions:
                output_parts.append(f"- {suggestion}")

    # å…ƒä¿¡æ¯
    output_parts.append(f"\n\n---\n*æ·±åº¦æ€è€ƒæ¨¡å¼ | LLMè°ƒç”¨æ¬¡æ•°: {result.total_llm_calls}*")

    return "\n".join(output_parts)
