"""
FastAPI æœåŠ¡ - æä¾› OpenAI æ ¼å¼å…¼å®¹çš„ LLM å®¢æˆ·ç«¯æ¥å£
"""

import json
import time
import uuid
from typing import List, Optional, Union, AsyncIterator

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from .api_service import api_service
from .config import PROVIDER_MODELS, get_model_provider

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="ThinkCloud API",
    description="OpenAI æ ¼å¼å…¼å®¹çš„å¤šæä¾›å•† LLM å®¢æˆ·ç«¯",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========== Pydantic æ¨¡å‹å®šä¹‰ ==========


class Message(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""

    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: system/user/assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©è¡¥å…¨è¯·æ±‚ï¼ˆOpenAI æ ¼å¼ï¼‰"""

    model: str = Field(..., description="æ¨¡å‹åç§°")
    messages: List[Message] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(default=None, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·å‚æ•°")
    max_tokens: Optional[int] = Field(default=None, gt=0, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    stream: bool = Field(default=False, description="æ˜¯å¦ä½¿ç”¨æµå¼ä¼ è¾“")
    frequency_penalty: Optional[float] = Field(
        default=None, ge=-2.0, le=2.0, description="é¢‘ç‡æƒ©ç½š"
    )
    presence_penalty: Optional[float] = Field(default=None, ge=-2.0, le=2.0, description="å­˜åœ¨æƒ©ç½š")
    stop: Optional[Union[str, List[str]]] = Field(default=None, description="åœæ­¢åºåˆ—")
    n: Optional[int] = Field(default=1, description="ç”Ÿæˆæ•°é‡")
    user: Optional[str] = Field(default=None, description="ç”¨æˆ·æ ‡è¯†")


class ChatCompletionResponseChoice(BaseModel):
    """èŠå¤©è¡¥å…¨å“åº”é€‰é¡¹"""

    index: int
    message: Message
    finish_reason: str  # stop, length, content_filter


class Usage(BaseModel):
    """Token ä½¿ç”¨ç»Ÿè®¡"""

    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    """èŠå¤©è¡¥å…¨å“åº”ï¼ˆOpenAI æ ¼å¼ï¼‰"""

    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: Usage


class ChatCompletionStreamChoice(BaseModel):
    """æµå¼å“åº”é€‰é¡¹"""

    index: int
    delta: dict  # {"role": "assistant", "content": "..."}
    finish_reason: Optional[str] = None


class ChatCompletionStreamResponse(BaseModel):
    """æµå¼å“åº”ï¼ˆOpenAI æ ¼å¼ï¼‰"""

    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[ChatCompletionStreamChoice]


class Model(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""

    id: str
    object: str = "model"
    created: int
    owned_by: str


class ModelList(BaseModel):
    """æ¨¡å‹åˆ—è¡¨"""

    object: str = "list"
    data: List[Model]


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""

    status: str
    providers: dict
    models_count: int


# ========== è¾…åŠ©å‡½æ•° ==========


def generate_id() -> str:
    """ç”Ÿæˆå”¯ä¸€ ID"""
    return f"chatcmpl-{uuid.uuid4().hex[:16]}"


def estimate_tokens(text: str) -> int:
    """ä¼°ç®— token æ•°é‡ï¼ˆç®€å•å®ç°ï¼‰"""
    # ç®€åŒ–ä¼°ç®—ï¼šä¸­æ–‡æŒ‰å­—ç¬¦æ•°ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯
    chinese_chars = sum(1 for c in text if "\u4e00" <= c <= "\u9fff")
    english_words = len(text.split())
    return chinese_chars + english_words


def format_openai_message(messages: List[Message]) -> List[dict]:
    """å°† Pydantic æ¨¡å‹è½¬æ¢ä¸ºå­—å…¸"""
    return [{"role": msg.role, "content": msg.content} for msg in messages]


# ========== API è·¯ç”± ==========


@app.get("/", tags=["åŸºç¡€"])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ThinkCloud API Server",
        "version": "1.0.0",
        "docs": "/docs",
        "openapi": "/openapi.json",
    }


@app.get("/health", response_model=HealthResponse, tags=["åŸºç¡€"])
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    providers_status = {}
    for provider_name in api_service.get_available_providers():
        providers_status[provider_name] = api_service.is_available(provider_name)

    # ç»Ÿè®¡æ‰€æœ‰å¯ç”¨æ¨¡å‹æ•°é‡
    total_models = sum(len(models) for models in PROVIDER_MODELS.values())

    return {
        "status": "healthy" if api_service.is_available() else "unhealthy",
        "providers": providers_status,
        "models_count": total_models,
    }


@app.get("/v1/models", response_model=ModelList, tags=["æ¨¡å‹"])
async def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    models = []
    timestamp = int(time.time())

    for provider_name, model_list in PROVIDER_MODELS.items():
        for model_id in model_list:
            models.append(
                {"id": model_id, "object": "model", "created": timestamp, "owned_by": provider_name}
            )

    return {"object": "list", "data": models}


@app.get("/v1/models/{model_id}", response_model=Model, tags=["æ¨¡å‹"])
async def retrieve_model(model_id: str):
    """è·å–æŒ‡å®šæ¨¡å‹ä¿¡æ¯"""
    provider_name = get_model_provider(model_id)

    if not provider_name:
        raise HTTPException(status_code=404, detail=f"æ¨¡å‹ '{model_id}' ä¸å­˜åœ¨")

    return {
        "id": model_id,
        "object": "model",
        "created": int(time.time()),
        "owned_by": provider_name,
    }


@app.post("/v1/chat/completions", tags=["èŠå¤©"])
async def create_chat_completion(request: ChatCompletionRequest):
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰

    å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼
    """
    # éªŒè¯æ¨¡å‹
    provider_name = get_model_provider(request.model)
    if not provider_name:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {request.model}")

    # æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
    if not api_service.is_available(provider_name):
        raise HTTPException(status_code=503, detail=f"æä¾›å•† '{provider_name}' ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®")

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    messages = format_openai_message(request.messages)

    # æµå¼å“åº”
    if request.stream:
        return StreamingResponse(
            stream_chat_completion(request, messages), media_type="text/event-stream"
        )

    # éæµå¼å“åº”
    return await non_stream_chat_completion(request, messages)


async def non_stream_chat_completion(
    request: ChatCompletionRequest, messages: List[dict]
) -> ChatCompletionResponse:
    """éæµå¼èŠå¤©è¡¥å…¨"""
    try:
        # è°ƒç”¨ API æœåŠ¡
        response_content = api_service.chat_completion(
            messages=messages,
            model=request.model,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            frequency_penalty=request.frequency_penalty,
            presence_penalty=request.presence_penalty,
            stream=False,
        )

        # ä¼°ç®— token ä½¿ç”¨é‡
        prompt_text = " ".join([msg["content"] for msg in messages])
        prompt_tokens = estimate_tokens(prompt_text)
        completion_tokens = estimate_tokens(response_content)

        # æ„é€  OpenAI æ ¼å¼å“åº”
        return ChatCompletionResponse(
            id=generate_id(),
            object="chat.completion",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionResponseChoice(
                    index=0,
                    message=Message(role="assistant", content=response_content),
                    finish_reason="stop",
                )
            ],
            usage=Usage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
            ),
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"API è°ƒç”¨å¤±è´¥: {str(e)}")


async def stream_chat_completion(
    request: ChatCompletionRequest, messages: List[dict]
) -> AsyncIterator[str]:
    """æµå¼èŠå¤©è¡¥å…¨"""
    try:
        # ç”Ÿæˆå”¯ä¸€ ID
        completion_id = generate_id()
        timestamp = int(time.time())

        # å‘é€åˆå§‹æ¶ˆæ¯ï¼ˆè§’è‰²å£°æ˜ï¼‰
        initial_chunk = ChatCompletionStreamResponse(
            id=completion_id,
            object="chat.completion.chunk",
            created=timestamp,
            model=request.model,
            choices=[
                ChatCompletionStreamChoice(
                    index=0, delta={"role": "assistant", "content": ""}, finish_reason=None
                )
            ],
        )
        yield f"data: {initial_chunk.model_dump_json()}\n\n"

        # è°ƒç”¨ API æœåŠ¡ï¼ˆæµå¼ï¼‰
        stream_generator = api_service.chat_completion(
            messages=messages,
            model=request.model,
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            frequency_penalty=request.frequency_penalty,
            presence_penalty=request.presence_penalty,
            stream=True,
        )

        # æµå¼å‘é€å†…å®¹
        for chunk_content in stream_generator:
            if chunk_content:
                chunk = ChatCompletionStreamResponse(
                    id=completion_id,
                    object="chat.completion.chunk",
                    created=timestamp,
                    model=request.model,
                    choices=[
                        ChatCompletionStreamChoice(
                            index=0, delta={"content": chunk_content}, finish_reason=None
                        )
                    ],
                )
                yield f"data: {chunk.model_dump_json()}\n\n"

        # å‘é€ç»“æŸæ¶ˆæ¯
        final_chunk = ChatCompletionStreamResponse(
            id=completion_id,
            object="chat.completion.chunk",
            created=timestamp,
            model=request.model,
            choices=[ChatCompletionStreamChoice(index=0, delta={}, finish_reason="stop")],
        )
        yield f"data: {final_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"

    except Exception as e:
        error_response = {
            "error": {
                "message": f"æµå¼ä¼ è¾“å¤±è´¥: {str(e)}",
                "type": "stream_error",
                "code": "stream_error",
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"


# ========== é”™è¯¯å¤„ç† ==========


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTP å¼‚å¸¸å¤„ç†"""
    return {
        "error": {"message": exc.detail, "type": "invalid_request_error", "code": exc.status_code}
    }


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """é€šç”¨å¼‚å¸¸å¤„ç†"""
    return {
        "error": {"message": f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(exc)}", "type": "server_error", "code": 500}
    }


# ========== å¯åŠ¨ä¿¡æ¯ ==========


@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶æ‰“å°ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ğŸš€ ThinkCloud FastAPI Server å¯åŠ¨æˆåŠŸï¼")
    print("=" * 60)
    print(f"ğŸ“– API æ–‡æ¡£: http://localhost:8000/docs")
    print(f"ğŸ”— OpenAPI Schema: http://localhost:8000/openapi.json")
    print(f"ğŸ’š å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print(f"ğŸ¤– å¯ç”¨æä¾›å•†: {', '.join(api_service.get_available_providers())}")
    print(f"ğŸ“Š æ¨¡å‹æ€»æ•°: {sum(len(models) for models in PROVIDER_MODELS.values())}")
    print("=" * 60 + "\n")
