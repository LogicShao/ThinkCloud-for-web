"""
ThinkCloud for Web - å¤šæä¾›å•† LLM å®¢æˆ·ç«¯
æ”¯æŒæ·±åº¦æ€è€ƒæ¨¡å¼çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
"""

from datetime import datetime

import gradio as gr

from src.api_service import api_service
from src.chat_manager import ChatManager, MessageProcessor
from src.config import (
    CHATBOT_HEIGHT,
    DEFAULT_MODEL,
    DEFAULT_SYSTEM_INSTRUCTION,
    MAX_INPUT_LINES,
    MODEL_PARAMETERS,
    SERVER_HOST,
    SERVER_PORT,
    check_api_key,
    get_server_port,
)
from src.deep_think import DeepThinkOrchestrator, format_deep_think_result


class LLMClient:
    """LLMå®¢æˆ·ç«¯ä¸»ç±»"""

    def __init__(self):
        self.chat_manager = ChatManager()
        self.message_processor = MessageProcessor()

    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        with gr.Blocks(title="ThinkCloud for Web - AI æ™ºèƒ½å¯¹è¯") as demo:
            # æ ‡é¢˜å’Œæè¿°
            gr.Markdown(self._get_header_markdown())

            # ä¸»è¦å†…å®¹åŒºåŸŸ
            with gr.Row(equal_height=True):
                # å·¦ä¾§æ§åˆ¶é¢æ¿
                with gr.Column(scale=1, min_width=280):
                    gr.Markdown("### ğŸ›ï¸ æ§åˆ¶ä¸­å¿ƒ")

                    # è·å–åˆ†ç»„çš„æ¨¡å‹æ•°æ®
                    from src.config import PROVIDER_DISPLAY_NAMES, get_enabled_providers

                    enabled_providers = get_enabled_providers()
                    provider_choices = [
                        PROVIDER_DISPLAY_NAMES.get(p, p.capitalize()) for p in enabled_providers
                    ]

                    # è·å–é»˜è®¤æä¾›å•†å’Œæ¨¡å‹
                    from src.config import get_model_provider

                    default_provider_id = get_model_provider(DEFAULT_MODEL)
                    default_provider_name = (
                        PROVIDER_DISPLAY_NAMES.get(
                            default_provider_id, default_provider_id.capitalize()
                        )
                        if default_provider_id
                        else provider_choices[0]
                    )

                    # ç¬¬ä¸€çº§ï¼šé€‰æ‹©æä¾›å•†
                    provider_dropdown = gr.Dropdown(
                        choices=provider_choices,
                        value=default_provider_name,
                        label="ğŸ¢ é€‰æ‹©æä¾›å•†",
                        info="é€‰æ‹©AIæœåŠ¡æä¾›å•†",
                        interactive=True,
                    )

                    # ç¬¬äºŒçº§ï¼šé€‰æ‹©æ¨¡å‹
                    from src.config import PROVIDER_MODELS

                    # è·å–é»˜è®¤æä¾›å•†çš„æ¨¡å‹åˆ—è¡¨
                    default_models = (
                        PROVIDER_MODELS.get(default_provider_id, []) if default_provider_id else []
                    )

                    model_dropdown = gr.Dropdown(
                        choices=default_models,
                        value=DEFAULT_MODEL
                        if DEFAULT_MODEL in default_models
                        else (default_models[0] if default_models else ""),
                        label="ğŸ¤– é€‰æ‹©æ¨¡å‹",
                        info="é€‰æ‹©å…·ä½“çš„AIæ¨¡å‹",
                        interactive=True,
                    )

                    # ç³»ç»ŸçŠ¶æ€æ˜¾ç¤ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰
                    gr.Markdown("### ğŸ“Š ç³»ç»ŸçŠ¶æ€")
                    status_html = gr.HTML(value=self._get_status_html())

                    # æ¨¡å‹å‚æ•°é…ç½®
                    gr.Markdown("### âš™ï¸ æ¨¡å‹å‚æ•°")

                    # æµå¼ä¼ è¾“æ§åˆ¶
                    enable_streaming = gr.Checkbox(
                        label="ğŸŒŠ å¯ç”¨æµå¼ä¼ è¾“", value=True, info="é€å­—æ˜¾ç¤ºå›å¤å†…å®¹ï¼ˆæ›´æµç•…çš„ä½“éªŒï¼‰"
                    )

                    # System Instruction
                    system_instruction = gr.Textbox(
                        label="ğŸ“ ç³»ç»Ÿæç¤ºè¯ (System Instruction)",
                        placeholder=DEFAULT_SYSTEM_INSTRUCTION,
                        value="",
                        lines=3,
                        max_lines=5,
                        info="ä¸ºæ¨¡å‹è®¾ç½®è§’è‰²å’Œè¡Œä¸ºè§„èŒƒï¼ˆç•™ç©ºä½¿ç”¨é»˜è®¤å€¼ï¼‰",
                    )

                    # Temperature æ»‘å—
                    temperature = gr.Slider(
                        minimum=MODEL_PARAMETERS["temperature"]["min"],
                        maximum=MODEL_PARAMETERS["temperature"]["max"],
                        value=MODEL_PARAMETERS["temperature"]["default"],
                        step=MODEL_PARAMETERS["temperature"]["step"],
                        label="ğŸŒ¡ï¸ Temperatureï¼ˆæ¸©åº¦ï¼‰",
                        info=MODEL_PARAMETERS["temperature"]["description"],
                    )

                    # é«˜çº§å‚æ•°æŠ˜å åŒº
                    with gr.Accordion("ğŸ”§ é«˜çº§å‚æ•°", open=False):
                        top_p = gr.Slider(
                            minimum=MODEL_PARAMETERS["top_p"]["min"],
                            maximum=MODEL_PARAMETERS["top_p"]["max"],
                            value=MODEL_PARAMETERS["top_p"]["default"],
                            step=MODEL_PARAMETERS["top_p"]["step"],
                            label="ğŸ¯ Top Pï¼ˆæ ¸é‡‡æ ·ï¼‰",
                            info=MODEL_PARAMETERS["top_p"]["description"],
                        )

                        max_tokens = gr.Slider(
                            minimum=MODEL_PARAMETERS["max_tokens"]["min"],
                            maximum=MODEL_PARAMETERS["max_tokens"]["max"],
                            value=MODEL_PARAMETERS["max_tokens"]["default"],
                            step=MODEL_PARAMETERS["max_tokens"]["step"],
                            label="ğŸ“ Max Tokensï¼ˆæœ€å¤§é•¿åº¦ï¼‰",
                            info=MODEL_PARAMETERS["max_tokens"]["description"],
                        )

                        frequency_penalty = gr.Slider(
                            minimum=MODEL_PARAMETERS["frequency_penalty"]["min"],
                            maximum=MODEL_PARAMETERS["frequency_penalty"]["max"],
                            value=MODEL_PARAMETERS["frequency_penalty"]["default"],
                            step=MODEL_PARAMETERS["frequency_penalty"]["step"],
                            label="ğŸ” Frequency Penaltyï¼ˆé¢‘ç‡æƒ©ç½šï¼‰",
                            info=MODEL_PARAMETERS["frequency_penalty"]["description"],
                        )

                        presence_penalty = gr.Slider(
                            minimum=MODEL_PARAMETERS["presence_penalty"]["min"],
                            maximum=MODEL_PARAMETERS["presence_penalty"]["max"],
                            value=MODEL_PARAMETERS["presence_penalty"]["default"],
                            step=MODEL_PARAMETERS["presence_penalty"]["step"],
                            label="âœ¨ Presence Penaltyï¼ˆå­˜åœ¨æƒ©ç½šï¼‰",
                            info=MODEL_PARAMETERS["presence_penalty"]["description"],
                        )

                    # æ·±åº¦æ€è€ƒæ¨¡å¼é…ç½®
                    gr.Markdown("### ğŸ§  æ·±åº¦æ€è€ƒæ¨¡å¼")
                    deep_think_enabled = gr.Checkbox(
                        label="å¯ç”¨æ·±åº¦æ€è€ƒ", value=False, info="ä½¿ç”¨å¤šé˜¶æ®µæ¨ç†æ·±å…¥åˆ†æé—®é¢˜"
                    )

                    with gr.Accordion("é«˜çº§é€‰é¡¹", open=False):
                        enable_review = gr.Checkbox(
                            label="å¯ç”¨è‡ªæˆ‘å®¡æŸ¥", value=True, info="å¯¹ç­”æ¡ˆè¿›è¡Œè´¨é‡å®¡æŸ¥"
                        )
                        show_thinking_process = gr.Checkbox(
                            label="æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹", value=True, info="å±•ç¤ºè¯¦ç»†çš„æ¨ç†æ­¥éª¤"
                        )
                        max_subtasks = gr.Slider(
                            minimum=3,
                            maximum=8,
                            value=6,
                            step=1,
                            label="æœ€å¤§å­ä»»åŠ¡æ•°",
                            info="é—®é¢˜æ‹†è§£çš„æœ€å¤§ä»»åŠ¡æ•°é‡",
                        )

                    gr.Markdown("""
                    ğŸ’¡ **åŠŸèƒ½æç¤º**

                    â€¢ æ”¯æŒ Markdown æ ¼å¼
                    â€¢ æ”¯æŒä»£ç é«˜äº®
                    â€¢ æ”¯æŒå¤šè½®å¯¹è¯
                    â€¢ å¯éšæ—¶åˆ‡æ¢æ¨¡å‹
                    â€¢ ğŸ§  æ·±åº¦æ€è€ƒæ¨¡å¼å¯è§£å†³å¤æ‚é—®é¢˜
                    """)

                # å³ä¾§èŠå¤©åŒºåŸŸ
                with gr.Column(scale=3, min_width=600):
                    # èŠå¤©ç•Œé¢
                    chatbot = gr.Chatbot(
                        label="ğŸ’¬ å¯¹è¯ç•Œé¢",
                        height=CHATBOT_HEIGHT,
                        type="messages",
                        show_copy_button=True,
                    )

            # è¾“å…¥åŒºåŸŸï¼ˆä¸ä¸Šæ–¹å¯¹é½ï¼‰
            with gr.Row():
                with gr.Column(scale=1, min_width=280):
                    pass  # å ä½ï¼Œä¸å·¦ä¾§æ§åˆ¶é¢æ¿å¯¹é½

                with gr.Column(scale=3, min_width=600), gr.Row():
                    msg = gr.Textbox(
                        label="âœï¸ è¾“å…¥æ¶ˆæ¯",
                        placeholder="ğŸ’­ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        scale=5,
                        max_lines=MAX_INPUT_LINES,
                        show_copy_button=False,
                        container=False,
                    )
                    submit_btn = gr.Button(
                        "ğŸš€ å‘é€", variant="primary", scale=1, size="sm", min_width=80
                    )

            # æ§åˆ¶æŒ‰é’®åŒºåŸŸï¼ˆä¸ä¸Šæ–¹å¯¹é½ï¼‰
            with gr.Row():
                with gr.Column(scale=1, min_width=280):
                    pass  # å ä½

                with gr.Column(scale=3, min_width=600), gr.Row():
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯", variant="secondary", size="sm", scale=1)
                    export_btn = gr.Button("ğŸ“¥ å¯¼å‡ºå¯¹è¯", variant="secondary", size="sm", scale=1)
                    gr.Markdown("*Powered by ThinkCloud*")

            # ç»‘å®šäº‹ä»¶
            self._setup_event_handlers(
                demo,
                msg,
                chatbot,
                provider_dropdown,
                model_dropdown,
                submit_btn,
                clear_btn,
                export_btn,
                status_html,
                enable_streaming,
                system_instruction,
                temperature,
                top_p,
                max_tokens,
                frequency_penalty,
                presence_penalty,
                deep_think_enabled,
                enable_review,
                show_thinking_process,
                max_subtasks,
            )

        return demo

    def _get_header_markdown(self):
        """è·å–å¤´éƒ¨Markdownå†…å®¹"""
        return """
        # ğŸš€ ThinkCloud for Web

        æ¢ç´¢ä¸‹ä¸€ä»£ AI å¯¹è¯ä½“éªŒ - æ”¯æŒå¤šæä¾›å•† + æ·±åº¦æ€è€ƒæ¨¡å¼

        ---

        **ğŸ¤– æ”¯æŒçš„æä¾›å•†:** Cerebras â€¢ DeepSeek â€¢ OpenAI â€¢ DashScope

        **âš¡ æ¨¡å‹ç³»åˆ—:** Llama â€¢ Qwen â€¢ DeepSeek â€¢ GPT

        **âœ¨ ç‰¹æ€§:** æ·±åº¦æ€è€ƒ â€¢ æ™ºèƒ½å‚æ•°è°ƒèŠ‚ â€¢ å¤šè½®å¯¹è¯
        """

    def _get_initial_status(self):
        """è·å–åˆå§‹çŠ¶æ€ä¿¡æ¯"""
        return api_service.get_provider_status()

    def _get_status_html(self):
        """ç”ŸæˆHTMLæ ¼å¼çš„ç³»ç»ŸçŠ¶æ€æ˜¾ç¤º"""
        from src.config import PROVIDER_DISPLAY_NAMES, get_enabled_providers

        enabled_providers = get_enabled_providers()
        history_count = self.chat_manager.get_history_length()

        # æ„å»ºæä¾›å•†åˆ—è¡¨
        provider_list = []
        for provider in enabled_providers:
            provider_name = PROVIDER_DISPLAY_NAMES.get(provider, provider.capitalize())
            provider_list.append(f"âœ“ {provider_name}")

        providers_text = ", ".join(provider_list)

        # æ„å»ºçŠ¶æ€HTML
        status_html = f"""
        <div>
            <p><strong>å¯ç”¨æä¾›å•†ï¼š</strong>{providers_text}</p>
            <p><strong>å¯¹è¯è½®æ•°ï¼š</strong>{history_count}</p>
        </div>
        """

        return status_html

    def _setup_event_handlers(
            self,
            demo,
            msg,
            chatbot,
            provider_dropdown,
            model_dropdown,
            submit_btn,
            clear_btn,
            export_btn,
            status_html,
            enable_streaming,
            system_instruction,
            temperature,
            top_p,
            max_tokens,
            frequency_penalty,
            presence_penalty,
            deep_think_enabled,
            enable_review,
            show_thinking_process,
            max_subtasks,
    ):
        """è®¾ç½®äº‹ä»¶å¤„ç†å™¨"""

        def update_models(provider_name):
            """å½“æä¾›å•†å˜æ›´æ—¶æ›´æ–°æ¨¡å‹åˆ—è¡¨"""
            from src.config import PROVIDER_DISPLAY_NAMES, PROVIDER_MODELS

            # ä»æ˜¾ç¤ºåç§°è·å–æä¾›å•†ID
            provider_id = None
            for pid, display_name in PROVIDER_DISPLAY_NAMES.items():
                if display_name == provider_name:
                    provider_id = pid
                    break

            # è·å–è¯¥æä¾›å•†çš„æ¨¡å‹åˆ—è¡¨
            models = PROVIDER_MODELS.get(provider_id, []) if provider_id else []

            # è¿”å›æ›´æ–°åçš„ä¸‹æ‹‰æ¡†é…ç½®
            return gr.update(choices=models, value=models[0] if models else "")

        def user_message(user_msg, history, model):
            """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
            if not user_msg.strip():
                return "", history

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.chat_manager.add_message("user", user_msg)

            # è·å–å½“å‰æ—¶é—´
            current_time = datetime.now().strftime("%H:%M:%S")

            # æ›´æ–°Gradioç•Œé¢ï¼Œåœ¨æ¶ˆæ¯ä¸­æ·»åŠ æ—¶é—´æˆ³
            new_history = [
                *history,
                {
                    "role": "user",
                    "content": user_msg,
                    "metadata": {"timestamp": current_time, "title": f"ğŸ• {current_time}"},
                },
            ]
            return "", new_history

        def bot_message(
                history,
                model,
                enable_stream,
                sys_inst,
                temp,
                top_p_val,
                max_tok,
                freq_pen,
                pres_pen,
                deep_think_mode,
                review_enabled,
                show_process,
                max_tasks,
        ):
            """è·å–æœºå™¨äººå›å¤"""
            if not history:
                yield history
                return

            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            last_user_msg = None
            for msg in reversed(history):
                if msg["role"] == "user":
                    last_user_msg = msg["content"]
                    break

            if not last_user_msg:
                yield history
                return

            # å¤„ç†ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            actual_sys_inst = sys_inst.strip() if sys_inst and sys_inst.strip() else None

            # è·å–å½“å‰æ—¶é—´
            start_time = datetime.now()
            time_str = start_time.strftime("%H:%M:%S")

            def format_duration(duration_seconds):
                """æ ¼å¼åŒ–æ—¶é—´å·®"""
                if duration_seconds < 1:
                    return f"{duration_seconds:.2f}s"
                elif duration_seconds < 60:
                    return f"{duration_seconds:.1f}s"
                else:
                    minutes = int(duration_seconds // 60)
                    seconds = int(duration_seconds % 60)
                    return f"{minutes}m {seconds}s"

            def add_duration_to_response(response, start_time):
                """åœ¨å›å¤å†…å®¹åº•éƒ¨æ·»åŠ å“åº”æ—¶é—´"""
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                duration_str = format_duration(duration)
                return f"{response}\n\n---\nâ±ï¸ **å“åº”æ—¶é—´:** {duration_str}"

            # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
            if deep_think_mode:
                # æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆæš‚ä¸æ”¯æŒæµå¼ä¼ è¾“ï¼‰
                try:
                    orchestrator = DeepThinkOrchestrator(
                        api_service=api_service,
                        model=model,
                        max_subtasks=int(max_tasks),
                        enable_review=review_enabled,
                        verbose=True,
                        system_instruction=actual_sys_inst,
                        temperature=temp,
                        top_p=top_p_val,
                        max_tokens=int(max_tok) if max_tok else None,
                    )

                    result = orchestrator.run(last_user_msg)

                    # æ ¼å¼åŒ–ç»“æœ
                    response = format_deep_think_result(result, include_process=show_process)

                except Exception as e:
                    response = (
                        f"æ·±åº¦æ€è€ƒæ¨¡å¼æ‰§è¡Œå¤±è´¥: {e!s}\n\nè¯·å°è¯•å…³é—­æ·±åº¦æ€è€ƒæ¨¡å¼æˆ–æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚"
                    )

                # æ·»åŠ å“åº”æ—¶é—´
                response = add_duration_to_response(response, start_time)

                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                self.chat_manager.add_message("assistant", response)

                # æ›´æ–°Gradioç•Œé¢ï¼ˆéæµå¼ï¼‰
                history.append(
                    {
                        "role": "assistant",
                        "content": response,
                        "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"},
                    }
                )
                # ä½¿ç”¨ yield è€Œä¸æ˜¯ returnï¼Œå› ä¸ºè¿™æ˜¯ç”Ÿæˆå™¨å‡½æ•°
                yield history

            else:
                # æ ‡å‡†æ¨¡å¼
                # æ„å»ºAPIæ¶ˆæ¯ - ç›´æ¥ä½¿ç”¨Gradioçš„historyæ ¼å¼
                api_messages = []
                for msg in history:
                    if msg["role"] in ["user", "assistant"]:
                        api_messages.append({"role": msg["role"], "content": msg["content"]})

                if enable_stream:
                    # æµå¼ä¼ è¾“æ¨¡å¼
                    # å…ˆæ·»åŠ ä¸€ä¸ªç©ºçš„åŠ©æ‰‹æ¶ˆæ¯
                    history.append(
                        {
                            "role": "assistant",
                            "content": "",
                            "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"},
                        }
                    )

                    response_text = ""
                    try:
                        # è°ƒç”¨APIï¼Œå¯ç”¨æµå¼ä¼ è¾“
                        stream_generator = api_service.chat_completion(
                            messages=api_messages,
                            model=model,
                            system_instruction=actual_sys_inst,
                            temperature=temp,
                            top_p=top_p_val,
                            max_tokens=int(max_tok) if max_tok else None,
                            frequency_penalty=freq_pen,
                            presence_penalty=pres_pen,
                            stream=True,
                        )

                        # é€æ­¥æ›´æ–°å›å¤
                        for chunk in stream_generator:
                            response_text += chunk
                            # æ›´æ–°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
                            history[-1]["content"] = response_text
                            yield history

                        # æµå¼ä¼ è¾“å®Œæˆï¼Œæ·»åŠ å“åº”æ—¶é—´
                        response_text = add_duration_to_response(response_text, start_time)
                        history[-1]["content"] = response_text
                        yield history

                    except Exception as e:
                        error_msg = f"æµå¼ä¼ è¾“å¤±è´¥: {e!s}"
                        error_msg = add_duration_to_response(error_msg, start_time)
                        history[-1]["content"] = error_msg
                        response_text = error_msg
                        yield history

                    # æ·»åŠ å®Œæ•´å›å¤åˆ°èŠå¤©å†å²ç®¡ç†å™¨
                    self.chat_manager.add_message("assistant", response_text)

                else:
                    # éæµå¼ä¼ è¾“æ¨¡å¼
                    try:
                        # è°ƒç”¨API
                        response = api_service.chat_completion(
                            messages=api_messages,
                            model=model,
                            system_instruction=actual_sys_inst,
                            temperature=temp,
                            top_p=top_p_val,
                            max_tokens=int(max_tok) if max_tok else None,
                            frequency_penalty=freq_pen,
                            presence_penalty=pres_pen,
                            stream=False,
                        )
                    except Exception as e:
                        response = f"APIè°ƒç”¨å¤±è´¥: {e!s}"

                    # æ·»åŠ å“åº”æ—¶é—´
                    response = add_duration_to_response(response, start_time)

                    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                    self.chat_manager.add_message("assistant", response)

                    # æ›´æ–°Gradioç•Œé¢
                    history.append(
                        {
                            "role": "assistant",
                            "content": response,
                            "metadata": {"timestamp": time_str, "title": f"ğŸ¤– {time_str}"},
                        }
                    )
                    # ä½¿ç”¨ yield è€Œä¸æ˜¯ returnï¼Œå› ä¸ºè¿™æ˜¯ç”Ÿæˆå™¨å‡½æ•°
                    yield history

        def clear_conversation():
            """æ¸…é™¤å¯¹è¯"""
            self.chat_manager.clear_history()
            return [], self._get_status_html()

        def export_conversation():
            """å¯¼å‡ºå¯¹è¯"""
            if not self.chat_manager.history:
                return self._get_status_html()

            export_text = "å¤šæä¾›å•† LLM å¯¹è¯è®°å½•\n" + "=" * 50 + "\n"
            for msg in self.chat_manager.history:
                role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
                export_text += f"{role}: {msg['content']}\n\n"

            return export_text

        def update_status():
            """æ›´æ–°çŠ¶æ€ä¿¡æ¯ï¼ˆHTMLæ ¼å¼ï¼‰"""
            return self._get_status_html()

        # æä¾›å•†å˜æ›´äº‹ä»¶ - æ›´æ–°æ¨¡å‹åˆ—è¡¨
        provider_dropdown.change(
            update_models, inputs=[provider_dropdown], outputs=[model_dropdown]
        )

        # ç»‘å®šäº‹ä»¶
        msg.submit(user_message, [msg, chatbot, model_dropdown], [msg, chatbot], queue=False).then(
            bot_message,
            [
                chatbot,
                model_dropdown,
                enable_streaming,
                system_instruction,
                temperature,
                top_p,
                max_tokens,
                frequency_penalty,
                presence_penalty,
                deep_think_enabled,
                enable_review,
                show_thinking_process,
                max_subtasks,
            ],
            [chatbot],
        ).then(update_status, None, [status_html])

        submit_btn.click(
            user_message, [msg, chatbot, model_dropdown], [msg, chatbot], queue=False
        ).then(
            bot_message,
            [
                chatbot,
                model_dropdown,
                enable_streaming,
                system_instruction,
                temperature,
                top_p,
                max_tokens,
                frequency_penalty,
                presence_penalty,
                deep_think_enabled,
                enable_review,
                show_thinking_process,
                max_subtasks,
            ],
            [chatbot],
        ).then(update_status, None, [status_html])

        clear_btn.click(clear_conversation, None, [chatbot, status_html], queue=False)

        export_btn.click(export_conversation, None, [status_html], queue=False)

        # é¡µé¢åŠ è½½æ—¶æ›´æ–°çŠ¶æ€
        demo.load(update_status, None, status_html)


def main():
    """ä¸»å‡½æ•°"""
    print("[START] å¯åŠ¨ ThinkCloud for Web...")

    # æ£€æŸ¥APIé…ç½®
    if not check_api_key():
        print("\n[WARN] è¯·å…ˆé…ç½®è‡³å°‘ä¸€ä¸ªAPIå¯†é’¥ç¯å¢ƒå˜é‡")
        print("   åˆ›å»º.envæ–‡ä»¶å¹¶æ·»åŠ ä»¥ä¸‹å˜é‡ä¹‹ä¸€:")
        print("   - CEREBRAS_API_KEY=your_api_key_here")
        print("   - DEEPSEEK_API_KEY=your_api_key_here")
        print("   - OPENAI_API_KEY=your_api_key_here")
        print("   - DASHSCOPE_API_KEY=your_api_key_here")
        print("\næ‚¨ä»ç„¶å¯ä»¥å¯åŠ¨ç•Œé¢ï¼Œä½†éœ€è¦é…ç½®APIå¯†é’¥æ‰èƒ½æ­£å¸¸ä½¿ç”¨ã€‚")

    # è‡ªåŠ¨æŸ¥æ‰¾å¯ç”¨ç«¯å£
    print("\n[PORT] æ£€æŸ¥ç«¯å£å¯ç”¨æ€§...")
    available_port = get_server_port(SERVER_PORT, SERVER_HOST)

    # åˆ›å»ºå¹¶å¯åŠ¨åº”ç”¨
    client = LLMClient()
    demo = client.create_interface()

    # å¯åŠ¨æœåŠ¡å™¨
    print("\n[LAUNCH] å¯åŠ¨WebæœåŠ¡å™¨...")
    print(f"   ä¸»æœº: {SERVER_HOST}")
    print(f"   ç«¯å£: {available_port if available_port else 'ç³»ç»Ÿåˆ†é…'}")
    print("   æµè§ˆå™¨å°†è‡ªåŠ¨æ‰“å¼€")
    print("=" * 60)

    demo.launch(
        server_name=SERVER_HOST,
        server_port=available_port,
        share=False,
        inbrowser=True,
        show_error=True,
    )


if __name__ == "__main__":
    main()
